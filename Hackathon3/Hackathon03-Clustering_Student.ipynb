{"cells":[{"cell_type":"markdown","metadata":{},"source":["<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n","<b>Hackathon 03 - Clustering: What is it all about?</b> </font> <br><br><br>\n","\n","<font size=5  color=#003366>\n","Prof. D. Hainaut<br>\n","Prof. L. Jacques<br>\n","\n","<br><br>\n","Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>\n","Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>\n","Antoine Legat (antoine.legat@uclouvain.be)<br>\n","Jérome Eertmans (jerome.eertmans@uclouvain.be)<br>\n","<div style=\"text-align: right\"> Version 2023</div>\n","\n","<br><br>\n","</font>"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\">\n","    This notebook is meant to be <b>displayed on a web browser</b>. If you are using Visual Studio, or anything else, things might not be displayed as expected. \n","</div>\n","\n","<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n","-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups, ...) must be clearly indicated!</b> (We can detect plagiarism)\n","-  This <b>notebook</b> (\".ipynb\" file) and the <b>report</b> (\".pdf\" file) must be delivered on <b>Gradescope</b> before **Friday, the 22nd of December 20h00**.\n","- We provide a <b>LaTeX template of the report</b> which can be found on Moodle. It is <b>mandatory to answer the questions in the spaces provided</b>. Gradescope will not work if answers are outside the boxes.\n","- The <b>grading</b> will be determined based on the <b>quality</b> of your <b>results</b> and your <b>report</b>. <b>Not</b> on the quality of your <b>code</b>. This <b>notebook</b> serves as a basis for the deliverables but it will <b>not</b> be <b>evaluated</b>.<br><br>\n","\n","<img src=\"Imgs/inaturalist.png\" width = \"600\">\n","\n","<div class=\"alert alert-danger\">\n","<b>[DELIVERABLE] Summary</b>  <br>\n","After the reading of this document (and playing with the code!), we expect you to provide us with:\n","<ol>\n","   <li> a PDF report written with the LaTeX template available on Moodle. It is <b>mandatory to keep the template as provided</b>;\n","   <li> and this Jupyter Notebook (it will not be read, just checked for plagiarism);\n","</ol>\n","\n","before <b>Friday, Friday the 22nd of December 22h00</b> on Gradescope.\n","\n","<b>Please</b>, export your image in <b>PDF</b> format, not JPEG or PNG, beforing including them in your report.<br> Exporting images in PDF is as simple as using the appropriate filename extension (see example below)!\n","</div>\n","\n","```python\n","# with matplotlib\n","plt.savefig(\"my_image.pdf\")\n","\n","# with plotly\n","fig.write_image(\"my_image.pdf\")\n","```\n","\n","\n","<font size=5 color=#009999> <b>CONTEXT & NOTEBOOK STRUCTURE</b> </font> <br>\n","    \n","The objective of this hackathon is threefold:\n","1. extract meaningful information from a dataset;\n","2. observe relationship(s) (if any) between features and eventual underlying groups (clusters);\n","3. and develop an unsupervised clustering tool and exploit the associated data.\n","\n","To this end, you will use a dataset (available on Moodle) inspired from the iNaturalist dataset that can be found on [GBIF](https://www.gbif.org/dataset/50c9509d-22c7-4a22-a47d-8c48425ef4a7). The provided version of the dataset only contains observations performed in Belgium and a reduced number of features, to avoid too large files.\n","\n","<font size=4 color=#AAAAAA> <b>HOW OBSERVATIONS ARE CLASSIFIED</b> </font> <br>\n","\n","iNaturalist is a platform that helps users identify the plants and animals around, while generating data for science and conservation, mostly thanks to their smartphone application. As a result, the iNaturalist dataset contains a large amount of observations (~73 000 000 as of 2023).\n","\n","In biology, individuals are classified using taxons. Taxons are organized in levels, from the **least** specific (*Life*) to the **most** (*Species*). The highest level is *Life*, as it contains the most individuals. In opposition, the lowest level is *Species*.\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a5/Biological_classification_L_Pengo_vflip.svg\" width = \"200\">\n","\n","The iNaturalist contains, for every observation, taxonomy levels from *Kingdom* to *Species*. It is important to understand that those levels create a hierarchy. Thus, knowing the *Class* of an individual fully determines all levels above: *Phylum*, *Kingdom*, etc.\n","\n","An example of taxonomy classification of three well known species is shown in the image below.\n","\n","<img src=\"https://dr282zn36sxxg.cloudfront.net/datastreams/f-d%3A816756bf99d9575eedaf35a4307be942a03a3bbf2b774ac7090999cc%2BIMAGE_TINY%2BIMAGE_TINY.1\" width = \"500\">\n","\n","<font size=4 color=#AAAAAA> <b>YOUR GOALS</b> </font> <br>\n","\n","Given a couple features and a given taxon level, `n`, you should be able to **create taxon-level clusters based on spatial-temporal coordinates and other provided features. Then, you must exploit the content of these different clusters to determine the likeliness of observating a given taxon level for some provided input requests, such as time, position, and the taxon levels above.**\n","\n","> **Example**. Imagine you are wandering near the Bois des Rêves at midnight, and you want to know the most likely animal *Order* your could observe among animals whose *Class* is *Mammalia*.\n","\n","\n","<div class=\"alert alert-danger\">\n","    Due to the hierarchical orgnization of taxons, it is important to understand that predicting a level <code>n</code> only makes sense if you do not have information about level <code>n-1</code> and lower.\n","    Trying to predict low taxonomy levels may be a difficult task for reasons you should determine yourselves. On the other side, predicting a very high level of taxonomy may not be interesting either. Choosing the right taxonomy level to predict is a challenging question that will highly impact your results! We will thus propose a <i>default</i> taxon level for the sake of simplicity.\n","\n","    default taxon-level: ORDER\n","</div>\n","\n","To synthesize, there are three main milestones: \n","\n","* **unsupervised clustering** of elements from the learning dataset based on some selected features;\n","* **supervised label assignment** of each formed cluster; e.g., name each cluster after the most appearing taxon within elements present in the cluster, taken at the <i>default</i> taxon level;\n","* and **supervised benchmarking** of the quality of this \"*clustering + assignment*\" procedure by trying to predict the taxon of new unseen elements (i.e., that were not part of the dataset used to build the clusters).\n","\n","This notebook is organized into parts. Each of them assesses one fundamental step to solve your problem:\n","\n","* PART 1 - DATA PREPROCESSING\n","   - 1.1 - Import the data\n","   - 1.2 - Features preprocessing\n","    <br><br>\n","* PART 2 - DATA VISUALIZATION\n","   - 2.1 - Dataset exploration\n","   - 2.2 - Spatial features visualization\n","   - 2.3 - PCA\n","    <br><br>\n","* PART 3 - IT'S TIME TO... CLUSTER!\n","   - 3.1 - Clustering : definition, example and execution\n","   - 3.2 - Results analysis\n","\n","Your answers to the questions in this notebook must be written in your report. We filled this notebook with preliminary (trivial) code. This practice makes possible to run most cells, without throwing warnings. <b>Take advantage of this to divide the work between all team members !</b>\n","\n","Finally, we strongly advise you to <b>read the whole notebook once</b> before jumping in.<br><br>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This allows you to show/hide the cells with code\n","# from https://mljar.com/blog/jupyter-notebook-hide-code/\n","\n","# WARNING: this might not work on Visual Studio Code\n","\n","from IPython.display import HTML\n","\n","HTML(\n","    \"\"\"<script>\n","code_show=true; \n","function code_toggle() {\n"," if (code_show){\n"," $('div.input').hide();\n"," } else {\n"," $('div.input').show();\n"," }\n"," code_show = !code_show\n","} \n","</script>\n","<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to show/hide the code.\"></form>\"\"\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°1: INSTALL DEPENDENCIES\n","Make sure you have everything installed\n","\"\"\"\n","\n","import sys\n","from IPython.utils import io\n","\n","with io.capture_output() as captured:\n","    !{sys.executable} -m pip install -r requirements.txt\n","\n","\n","if \"ERROR\" in captured.stdout:\n","    print(captured.stdout)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%javascript\n","IPython.OutputArea.prototype._should_scroll = function(lines) {\n","    return false;\n","}"]},{"cell_type":"markdown","metadata":{},"source":["<br><font size=7 color=#009999> <b>PART 1 - DATA PREPROCESSING</b> </font> <br><br>\n","\n","<font size=5 color=#009999> <b>1.1 - IMPORT THE DATA</b></font>\n","    \n","    \n","### iNaturalist dataset \n","\n","\n","\n","In this dataset, you are provided some data related to observations of individuals, e.g., animals or plants. \n","\n","This includes, apart from the aforementioned taxons, geolocation, temporal and taxon information:\n","\n","- `eventDate`: refers to YYYY-MM-DD T HH:MM:SS the moment at which the observation has been made;\n","- `decimaLatitude`: latitude of the observation;\n","- `decimalLongitude`: longitude of the observation;\n","- `stateProvince`: in which \"macro\" part of Belgium the observation has been recorded.\n","\n","Take a look at the next cell to see the exact composition of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["\"\"\"\n","CELL N°2: IMPORT THE DATASET\n","\n","IMPORTANT: we directly split the dataset into a train and a test set. Don't forget\n","to use `df_test` to evaluate your model at the very end!\n","\"\"\"\n","from collections import Counter\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","import toolbox as tb\n","\n","from IPython.display import display, Markdown as md\n","from PIL import Image\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets, preprocessing\n","\n","pd.set_option(\"max_colwidth\", 200)\n","\n","path = \"Data/inaturalist_be.csv\"\n","df = pd.read_csv(path, parse_dates=[\"eventDate\"])\n","df.set_index(\"gbifID\", inplace=True)\n","df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n","df, df_test = train_test_split(df, test_size=0.1)\n","\n","# Print first rows\n","display(md(\"# iNaturalist\"))\n","display(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["## TAXONOMY ##\n","\n","taxon_hierarchy = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n","\n","fixed_taxons = [(\"class\", \"Insecta\")]  # You restrict the dataset to a sub-sample of '...'\n","\n","# fixed_taxons = [(\"kingdom\", \"Plantae\")]  # Other example of restriction\n","\n","for taxon_name, taxon_value in fixed_taxons:\n","    df = df[df[taxon_name] == taxon_value]\n","\n","target_taxon = \"order\"  # What you want to predict, we also suggest trying 'family'\n","target_taxon_index = list(df.columns.values).index(target_taxon)  # Compute the index `n`"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<font size=5 color=#009999> <b>1.2 - FEATURES PREPROCESSING</b> <br>\n","REMOVING UNNECESSARRY INFORMATION, CLEANING DATASET AND CREATING NEW FEATURES\n","</font> <br> <br>\n","\n","When doing data sciences, the datasets you are using are most probably not made for your very application. Instead, they result from the collection of information throughout a certain period of time, and it is the data scientist's job to make a good use of those datasets.\n","\n","For this hackathon, your goal is to **determine a new individual observation's most likely ORDER based on position and time requests as well as eventual other information**. By default, we restrict the dataset to **only contain insects** (see `fixed_taxons` variable). Therefore, you should be able to determine which features are useful for your application.\n","\n","<div class=\"alert alert-warning\">\n","<b>[Question 1.1] Removing unnecessary features </b>  <br>\n","Can you already, a priori, detect that some features are useless?\n","<ol>\n","   <li> if yes, list those (useless) features and explain your choice;\n","   <li> if not, then explain why it is better to wait.\n","</ol>\n","    Generally speaking, is it a good idea to remove a feature based on <i>a priori</i> knowledge, or doesn't it alter the final outcome?\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°3: REMOVING UNNECESSARY FEATURES\n","If you answered yes in the previous question, please remove those features here.\n","\n","WARNING: removing some features might prevent the following cells to run (especially visualization cells).\n","Please adapt your code accordingly.\n","\"\"\"\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","features_to_drop = [\n","    # Fill the list here or leave empty\n","]\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","df.drop(labels=features_to_drop, axis=1, inplace=True, errors=\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<b>[Remark 1.1]</b><br>\n","In most real-cases, the datasets you are going to work with will contain artifacts, such as typos or missing data, that you may want to remove before feeding the data into any algorithm. Here, Pandas treats missing data as \"NaNs\" (refering to Not a Number, even though it is used for every missing object, not for numbers only).\n","</div>\n","\n","Can you find a way to inspect your dataset and see if there are some missing data?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°4: INFORMATION ABOUT TYPES AND NANs\n","Use one (or more) of Pandas' builtin functions to get information about data types\n","and the number of missing (NaN) values for each feature.\n","\"\"\";\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<b>[Remark 1.2] Each problem has its own solution</b> <br>\n","There exists numerous ways to deal with missing information and we will discuss the two main approaches:\n","<ol>\n","   <li> you remove rows or columns that contain missing data;\n","   <li> or you replace NaNs with another value. The latter can be a fixed value or computed to be, e.g., the mean of all non-NaNs values. The topic of replacing missing data, also called imputation of missing values, is very broad and complex, and there is no global solution that applies everywhere. Maybe you can find one that works well here?\n","</ol>\n","    \n","You **should** read more about how to imput missing value [here](https://scikit-learn.org/stable/modules/impute.html).\n","</div> \n","\n","<div class=\"alert alert-warning\">\n","<b>[Question 1.2] Handling missing data </b>  <br>\n","Given the dataset and the amount / type of missing information, what strategy do you propose to follow regarding missing data (NaNs)? <br> You can choose one or many of the following:\n","<ol>\n","   <li> drop features (column) with missing information; \n","   <li> drop samples (row) with missing information;\n","   <li> replace missing information with interpolation / extrapolation / simple substitution / ...\n","</ol>\n","Justify briefly your choice.\n","</div> "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°5: HANDLING NANs\n","Apply your handling NaN strategy by filling the gaps according to what you have decided to do.\n","Do not reinvent the wheel: a good data-scientist is a lazy d-s.\n","Feel free to modify the code.\n","\"\"\"\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","# This is an example of how you can drop columns or rows containing NaNs,\n","# feel free to modify the code if your prefer.\n","\n","drop_rows = [\n","    # Fill the list here or leave empty\n","]\n","# set `drop_rows = None` to check for all rows\n","# set `drop_rows = []` to check for no rows\n","\n","drop_cols = [\n","    # Fill the list here or leave empty\n","]\n","# set `drop_cols = df.columns` to select all columns\n","# set `drop_rows = []` to select no column\n","\n","# Fill special substitution here (e.g., every NaN becomes an empty string \"\")\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","df.dropna(\n","    axis=0, subset=drop_rows, inplace=True\n",")  # Drop rows with NaN in any of the mentionned columns\n","\n","for col in drop_cols:\n","    # For each mentionned column, drop it if it contains `any` NaN\n","    if df[col].isnull().values.any():\n","        df.drop(col, axis=1, inplace=True)\n","\n","df"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<b>[Remark 1.3] New features extraction</b> <br>\n","In the present case, some features in the dataset still need to be reworked in order to provide meaningful information. For example, working with datetimes might not be easy.\n","</div>\n","\n","You may want to somehow incorporate the information about date and time into the dataset in a more **intelligent** manner than it was before. Again, there can be multiple solutions, and we will propose you a very simple one.\n","\n","For example, you may be interested only in the day of an observation, not the exact second. Other features can also be created, and it is up to you to find the best ones! \n","\n","The following code will show you how to get the day. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°6: CREATING FEATURES\n","Here is how you can extract the date from the datetime feature.\n","You can create here your own ones!\n","\"\"\"\n","\n","from datetime import datetime\n","\n","try:\n","    # E.g., get day information (as a number)\n","    df[\"day\"] = df[\"eventDate\"].dt.day\n","except:\n","    pass\n","\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","# Add the features you want\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","display(\n","    md(\n","        \"#### Here is how your dataset looks like now that you have added your new feature(s):\"\n","    )\n",")\n","df"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 1.3] New features </b>  <br>\n","What features have you added? If a particular manipulation has been applied, please explain.\n","</div> \n","\n","<br><font size=7 color=#009999> <b>PART 2 - DATA VISUALIZATION</b> </font> <br><br>\n","\n","Now that we are done with handling and preprocessing the data, we will further dig into the dataset to see what it actually contains.\n","\n","<br><font size=5 color=#009999> <b>PART 2.1 - DATASET EXPLORATION</b> </font> <br><br>\n","\n","Bringing an **understanding of data** to data scientists **is crucial**. Visualizing the distributions of each feature generally provides a good insight into how that data is distributed. \n","\n","There exists plenty of ways for visualizing large amount of data. Indeed, data visualization is a whole branch of data science which aims to make accessible the ways to view and understand data.\n","\n","Hopefully, there also exists many amazing tools that allow you to plot data efficiently. We provide you with two simple examples of those (note that we sometimes subsample the dataset to avoid plots with a too large amount of points, which can be slow on some computers)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°7: OBSERVATIONS PER DATETIME\n","\"\"\"\n","\n","fig = df.sample(frac=0.01, random_state=1234)[\"eventDate\"].hist(\n","    backend=\"plotly\", title=\"count of #observations per datetime\"\n",")\n","\n","fig.update_layout(\n","    xaxis_title=\"Date\",\n","    yaxis_title=\"Count\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°8: WORDCLOUD\n","\"\"\"\n","\n","counter = Counter(df[target_taxon])\n","tb.word_cloud(\n","    counter,\n","    title=\"WordCloud of \"\n","    + str(target_taxon)\n","    + \" names - bigger means more occurences\",\n",")\n","\n","\n","print(\n","    \"There are \"\n","    + str(df[target_taxon].nunique())\n","    + \" different observations at this taxon level.\"\n",")\n","\n","counter"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 2.1] Features visualization </b> <br>\n","    \n","Based on what you have seen above and what you will try, you can already get an idea of which features seem to contain discriminative information, i.e., which features are likely to be more important for the clustering than others. Please, explain.\n","    \n","Justify which features you think would be interesting or not to keep in order to reach your goals.\n","Feel free to try and add your own data visualization to highlight their importance (or not).\n","</div>\n","\n","<div class=\"alert alert-info\">\n","<b>[Remark 2.1] Features visualization</b> <br>\n","As mentioned before, there are many ways to visualize data. \n","    \n","Do not hesitate to support your arguments with original plots to illustrate your point. Feel also free to illustrate the potential new features you have introduced earlier.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°9: YOUR OWN VISUALIZATIONS\n","Feel free to create more plots here (or elsewhere).\n","\"\"\";"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<b>[Remark 2.2] Feature pruning</b> <br>\n","Based on your previous visualizations and choices, reuse the code at the top of the notebook to drop features that are unnecessary. \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°10: REMOVING UNNECESSARY FEATURES\n","You can use this cell to drop other features (if any) you consider unnecessary.\n","\"\"\";\n"]},{"cell_type":"markdown","metadata":{},"source":["<br><font size=5 color=#009999> <b>PART 2.2 - SPATIAL FEATURES VISUALIZATION</b> </font> <br><br>\n","\n","As our dataset contains spatial coordinates for each observation, it is therefore imperative to correctly visualize the data concerning the localization of these latter.\n","\n","Since this location information are based on real coordinates, we will be able to visualize this data on real maps. We therefore provide a set of tools that will allow you to observe the distribution of this data on a world map and, more precisely, in Belgium."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°11: GEOGRAPHICAL VISUALIZATION\n","\n","Read the code carefully and try to understand it. Then, play with it! \n","\"\"\"\n","\n","sub_df = df.sample(frac=0.1, random_state=1234)\n","\n","brussels = {\n","    \"lat\": 50.8476,\n","    \"lon\": 4.3572,\n","}\n","try:\n","    fig = px.scatter_mapbox(\n","        sub_df,\n","        lat=sub_df[\"decimalLatitude\"],\n","        lon=sub_df[\"decimalLongitude\"],\n","        color=target_taxon,  # which column to use to set the color of markers\n","        hover_name=target_taxon,  # column added to hover information\n","        center=brussels,\n","        mapbox_style=\"open-street-map\",\n","        color_discrete_sequence=px.colors.qualitative.Dark24,\n","    )\n","\n","    fig.show()\n","\n","except:\n","    print(\"data necessary for the plot has been removed in a previous cell\")"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 2.2] Spatial features visualization </b> <br>\n","\n","Based on the maps above, what can you infer about the locations of the individuals?\n","</div>\n","\n","\n","<br>\n","<font size=5 color=#009999> <b>2.3 - PCA</b> <br>\n","REDUCE THE DIMENSIONALITY OF THE DATA IN ORDER TO OBSERVE IT\n","</font> <br> <br>\n","\n","<!---\n","The high dimensionality of the dataset (number of columns) makes data visualization hard. In order to gain some (partial) information about our data distribution,\n","--->\n","PCA is often considered as the simplest and most fundamental technique used in dimensionality reduction. Remember that PCA is essentially the rotation of coordinate axes, chosen such that each successful axis captures or preserves as much variance as possible. If the algorithm returns a new system coordinates of the same dimension as the input, we can keep only the axis corresponding to the 3 largest singular values and project data on this coordinates system to perform the visualization.\n","\n","\n","![PCAUrl](https://miro.medium.com/max/400/1*ZXhPoYQIn-Y8mxoUpz5Ayw.gif \"PCA\")\n","\n","Although PCA allows to reduce dimensionality for a visualization purposes, it can also give information about the *importance of our features*. Indeed, some of them contribute more to the choice of the principal components and are therefore those that best explain the dataset.\n","\n","To vizualize the importance of features, we can extract the [PCA loadings](https://scentellegher.github.io/machine-learning/2020/01/27/pca-loadings-sklearn.html). These are indicators of the correlation between components and original features. The value of loadings is contained between -1 and 1. The more the value goes toward those boundaries, the more the feature influences the choice of component.  \n","We propose to perform a 2-dimensional PCA and then to add the loadings in vector form to the figure to obtain what is called a *biplot* (see example on the iris dataset below). \n","\n",">**NOTE**: even if the figure below resembles the previous one, the former displays the directions of the 3 principal components while this one shows the importance of each feature."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°12: LOADINGS EXAMPLE ON A SEPARATE TOY DATASET\n","Plot of the reduced iris dataset to 2 dimensions with the loadings\n","(i.e. vectors that represent the importance of each feature)\n","\n","There is nothing to do here ;-)\n","\"\"\"\n","iris = datasets.load_iris()\n","\n","names = iris.target_names\n","\n","X = preprocessing.scale(iris.data)\n","y = [names[i] for i in iris.target]\n","\n","pca = PCA(n_components=2)\n","X_data2 = pca.fit_transform(X)\n","\n","tb.biplot_visualization(pca, X_data2, y, columns=iris.feature_names)"]},{"cell_type":"markdown","metadata":{},"source":["Knowing that you will have to predict a taxon level from spatial and temporal coordinates, and optionally other information, you first need to split your dataset into a feature vector (`X`) and a target vector (`y`), then to reduce dimensionality of the feature space to compute and vizualize the loadings.\n","\n","<div class=\"alert alert-warning\">\n","<b>[Question 2.3] Feature importance visualization </b>  <br>\n","    Use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?highlight=pca#sklearn.decomposition.PCA\">PCA</a> function from scikit learn to reduce your data to 2 dimensions. Then, use the <samp>biplot_visualization()</samp> function provided in the toolbox (<i>toolbox.py</i>) to vizualize the biplot graph.\n","\n","Do all features have the same importance? If no, which features are less important, and why?\n","You can use all other graphs from the visualization part to justify your answer.\n","</div>\n","\n","<div class=\"alert alert-info\">\n","<b>[Remark 2.3] </b> <br>\n","Feel free to try this vizualization with numerical features you already dropped (if any) to support your previous choices.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":["\"\"\"\n","CELL N°13: FEATURE IMPORTANCE VISUALIZATION\n","\n","IMPORTANT: only keep numerical features in X (no NaNs).\n","If needed, you can use a mapping function,e.g., from string\n","to numerical values. Dates are not numbers!\n","\n","You can also select all numerical features with:\n","    df.select_dtypes(include=np.numbers)\n","\"\"\"\n","\n","# First, we recommend using a fraction of the datatest\n","# and you should remove this (of set frac=1) when you\n","# feel confident about your model\n","\n","sub_df = df.sample(frac=0.1, random_state=1234)\n","\n","output = target_taxon\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","y = ...  # target output\n","X = ...  # input DataFrame, without output column(s)\n","columns = ...  # a vector of input feature names, usually `X.columns`\n","\n","# Standardize the data\n","X = ...  # do some standardization\n","\n","# Apply a 2-dimensional PCA\n","pca = ...\n","X_data2 = ...\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","if all(not isinstance(x, type(...)) for x in [pca, X_data2, columns]):\n","    fig = tb.biplot_visualization(pca, X_data2, y, columns=columns)\n","    fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<b>[Remark 2.4] Feature pruning</b> <br>\n","Based on your previous conclusions you may want to prune other features. If so, reuse the code at the top of the notebook to drop those features. \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°14: REMOVING UNNECESSARY FEATURES\n","You can use this cell to drop other features you consider unnecessary.\n","\"\"\";\n"]},{"cell_type":"markdown","metadata":{},"source":["<br> <br>\n","<font size=7 color=#009999> <b>PART 3 - IT'S TIME TO... CLUSTER!</b> </font> <br><br>\n","\n","<br>\n","<font size=5 color=#009999> <b>3.1 - Clustering: definition, example and execution</b> <br>\n","THE ABC OF CLUSTERING\n","</font> <br> <br>\n","\n","Clustering can be defined as the task of *grouping* objects from a set $S$ (here, each row/observation is an object) in such a way that objects assigned to the same group (called cluster) are more **similar** (or less **distant**) with respect to each other (in some sense) than to those assigned to the other groups. Usually, we would like to divide our objects into $K$ groups.\n","\n","As such, clustering reduces to finding, among all $K$-partitions possible of $S$, the partition $\\mathcal{P}$ that minimizes some error criterion $f(\\mathcal{P})$. Each object will be assigned a cluster, $C_i$, and each cluster will have its centroid $c_i$ the distance between **any object** in $C_i$ to centroid $c_i$ is **always smaller** that the distance to any other centroid. In other words, each object is assigned to the cluster whose centroid is the closest.\n","\n","\n","A mathematical formulation of the problem could be the following, $$ \\boxed{\\min_{(C_1,\\dots,C_K) \\,\\in\\, \\mathcal{P}}\\,f(C_1,\\dots,C_K) = \\sum_{i = 1}^{K}\\,\\sum_{x \\in C_i}\\,\\Delta(x,c_i)}$$\n","\n","where $\\Delta(x,c_i)$ denotes the distance between object $x$ and centroid $c_i$.\n","\n","<br>\n","<font size=5 color=#009999>\n","EXAMPLE OF SEPARATING OBJECTS INTO 10 CLUSTERS\n","</font> <br> <br>\n","\n","**First**, let us imagine the following 2D dataset.\n","\n","<img src=\"Imgs/10-partitions-data.svg\" width = \"250\">\n","\n","**Then**, a 10-partition is defined by the position of the centroids, one for each cluster. Below, you can observe four examples of (random) centroids localizations (stars).\n","\n","<img src=\"Imgs/10-partitions-chose-centroids.svg\" width = \"1000\">\n","\n","**Next**, the regions are colored based on their closest centroid. Here, we take the distance to be the Euclidean distance.\n","\n","<img src=\"Imgs/10-partitions-centroids.svg\" width = \"1000\">\n","\n","**Finally**, data points (objects) are colored based in the region they are in.\n","\n","<img src=\"Imgs/10-partitions-clusters.svg\" width = \"1000\">\n","\n","\n","As a reminder, for this hackathon, your goal is to determine the taxon level that is the most likely to observe based on some input data.\n","\n","To do so, you need first to <b>cluster your data</b>, then identify <b>the closest cluster to each element in your test set</b>, and finally **return a list a observation probabilities for each taxon variant**. Of course, the list of probabilities must sum to one!\n","\n","A very easy way to generate such list is to **count the occurences** of each taxon variant in a cluster, and then **divide** each count by the number of observations in that cluster:\n","\n","```python\n","from collections import Counter\n","\n","observations_in_cluster = ... # This is pseudocode\n","\n","count_per_variant = Counter(observations_in_cluster)\n","count = sum(variant_count for variant_count in count_per_variant.values())\n","probabilities = {\n","    variant_name: variant_count / count\n","    for variant_name, variant_count in count_per_variant.items()\n","}\n","```\n","\n","Then, the classificaton score will be simple evaluated as:\n","\n","```python\n","true_taxon_variant = ...\n","\n","score = probabilities.get(true_taxon_variant, 0.0)  # The highest score is 1.0\n","```\n","\n","<div class=\"alert alert-info\">\n","<b>[Remark 3.1] </b> <br>\n","Here, we use probabilities rather that true prediction to avoid penalizing too much when a given cluster may contain multiple taxon variants with similar probabilities.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°15: GROUND TRUTH\n","Create your own ground truth vector (i.e., split your dataset in train and validation sets)\n","Reuse X (standardized) and y from previous cell to create those sets.\n","\"\"\"\n","\n","train_indices, val_indices = tb.train_val_indices(X.shape[0], val_frac=0.1, seed=1234)\n","\n","X_train, X_val = X[train_indices, :], X[val_indices, :]\n","y_train, y_val = y.values[train_indices], y.values[val_indices]"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 3.1] Number of clusters </b>  <br>\n","    Accounting for all features (i.e., spatial <b>and</b> temporal coordinates), what do you think is the ideal number of clusters? What will happen if too many or even too few clusters are chosen?\n","</div>\n","\n","Now that your dataset is divided into a train and a validation set, use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans</a> algorithm from `scikit-learn` to apply the clustering on your dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false},"outputs":[],"source":["\"\"\"\n","CELL N°16: KMEANS CLUSTERING\n","Use KMeans to group your data into k clusters\n","\"\"\"\n","\n","k = 50  # initial value, you can replace it by whatever you prefer\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","kmeans = ...\n","cluster_y_train = ...  # predict the cluster index for each sample of X_train\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","# We create a DataFrame from train data, that will contain the cluster of each entry\n","X_train_df = pd.DataFrame(data=X_train, columns=columns)\n","X_train_df[\"cluster\"] = cluster_y_train\n","X_train_df[output] = y_train\n","X_train_df"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 3.2] Cluster composition </b>  <br>\n","Currently, we suggest returning a list of probabilities, not a true unique prediction.\n","How could you return a prediction <code>y_pred</code> given an input observation vector <code>x</code>?\n","Also, what do you think would be the best way to do so (based on a simple binary score function)? Explain.\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°17: PREDICTING TAXON VARIANT\n","Return the taxon variant probabilities for each element in your validation set.\n","\n","You are encourage to vary:\n","- the number of clusters\n","- the predicted taxon level - (see remark 3.3)\n","\"\"\"\n","\n","propabilities_per_cluster = {\n","    cluster_id: (cluster_df[output].value_counts() / cluster_df.shape[0]).to_dict()\n","    for cluster_id, cluster_df in X_train_df.groupby(\"cluster\")\n","}\n","\n","propabilities_per_cluster\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","# add more tests here\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<br>\n","<font size=5 color=#009999> <b>3.2 - RESULTS ANALYSIS </b> <br>\n","OBSERVE AND COMPARE\n","</font> <br> <br>\n","\n","In this section, we adress the difficult task of evaluating the performance of the clustering algorithm.\n","\n","\n","<br>\n","<b>1. Use a metric function.</b> One way to assess the quality of your data partitionning is to use a metric that compares the predicted vector with the true one according to a well chosen function. For this hackathon, you can find in <i>toolbox.py</i>, the function <samp>accuracy_metric()</samp> that contains the metric we will use to evaluate your solution. The value of the metric lies between 0 and 1, 0 being the worse case and 1 the best.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°18: METRIC\n","\n","Compute the metric on the validation set.\n","\"\"\"\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","cluster_ids = kmeans.predict(X_val)\n","\n","y_pred = [propabilities_per_cluster[cluster_id] for cluster_id in cluster_ids]\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n","\n","accuracy = tb.accuracy_metric(y_val, y_pred)\n","\n","print(\n","    \"Prediction on\",\n","    output,\n","    \"resulted in an average accuracy of\",\n","    np.mean(accuracy),\n","    f\"(std.: {np.std(accuracy)})\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["<b>2. Compare different solutions.</b> An important thing to do while looking for an optimal solution is to compare the impact of your changes. Since you have access to the metric function, you should be able to assess the evolution of the metric value according to your choice of hyperparameters, methods, etc.  \n","> **NOTE:**  graphs are great tools for comparing solutions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°19: COMPARE METHODS\n","\"\"\";\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","# Feel free to use this cell to implement your way of comparing your methods\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["**3. Evaluate your solution on unknown data.** Now that you have made all the choices regarding your clustering solution and your model is ready, you need to evaluate it on new data (i.e., not used during the training step). We provide you with a test set containing new observations. Apply your method to this data and use the metric function to evaluate the results.  \n","> **NOTE:** do not forget all the preprocessing steps you had to do on the training set before using it as input to your clustering model. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","CELL N°20: EVALUATE ON TEST DATA\n","Whenever you feel that your model is ready, you should evaluate on unkwown test data.\n","\"\"\"\n","\n","df_test\n","\n","#########################################################################################################\n","# Start : Student version\n","#########################################################################################################\n","\n","# evaluate your model(s) on the test data\n","\n","#########################################################################################################\n","# End : Student version\n","#########################################################################################################"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<b>[Question 3.3] Your clustering solution </b> <br> \n","Describe here your clustering solution (how many clusters and other important choices that have been made, etc.). Justify your choices with the help of the metric.\n","</div>\n","\n","<div class=\"alert alert-info\">\n","<b>[Remark 3.2] </b> <br>\n","Do not hesitate to accompany your explanation with some graphs. You can support your answer by ploting, by example, the evolution of the quality of your predictions according to the number <code>k</code> of clusters.\n","</div>\n","\n","<div class=\"alert alert-info\">\n","<b>[Remark 3.3] </b> <br>\n","    By default, the target output is set to the taxon level <code>ORDER</code>. What happens if you change the predicted taxon level to another, e.g., <code>FAMILY</code>? Question 3.4 is dedicated to this comparison.\n","</div>\n","\n","<div class=\"alert alert-warning\">\n","<b>[Question 3.4] Comparing models - BONUS</b> <br> \n","Compare how your model performs when predicting other taxon levels. I.e., does predicting taxonomy at a different level performs better or worse than the default?\n","</div>\n"]},{"cell_type":"markdown","metadata":{},"source":["<font size=5 color=#009999>\n","WELL DONE!\n","</font>\n","\n","Now, you should be able to create a complete pipeline, from start to end, that can train a model on a data, and output some prediction for a given input vector. This will be seen during, e.g., the [LELEC2870 - Machine learning : regression, deep networks and dimensionality reduction](https://uclouvain.be/en-cours-2021-lelec2870) classes."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"6ba07c751260516eeb73d18fac358a7693501f65ad6727169b82411af5185fde"}}},"nbformat":4,"nbformat_minor":2}
