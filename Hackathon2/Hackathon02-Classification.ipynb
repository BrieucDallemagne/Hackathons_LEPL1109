{"cells": [{"cell_type": "markdown", "id": "dd99b142", "metadata": {}, "source": ["<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> \n", "<b>Hackathon 02 - Classification: Stars, Galaxies and Quasars </b> </font> <br><br><br>\n", "\n", "<font size=5  color=#003366>\n", "Prof. D. Hainaut<br>\n", "Prof. L. Jacques<br>\n", "\n", "<br><br>\n", "Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>\n", "Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>\n", "J\u00e9rome Eertmans (jerome.eertmans@uclouvain.be)<br>\n", "Antoine Legat (antoine.legat@uclouvain.be)<br>\n", "<div style=\"text-align: right\"> Version 2023</div>\n", "\n", "<br><br>\n", "</font>"]}, {"cell_type": "markdown", "id": "f8fe2cb4", "metadata": {}, "source": ["<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>\n", "-  This assignment is due on the <b>04 December 2023 at 22h00</b>.\n", "-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, ChatGPT, other groups,...) must be clearly indicated!</b>\n", "-  This notebook (with the \"ipynb\" extension) file, the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.\n", "- Only the PDF report will be graded on content and quality of the text / figures. <br><br>\n", "\n", "<div class=\"alert alert-danger\">\n", "<b>[DELIVERABLE] Summary</b>  <br>\n", "After the reading of this document (and playing with the code!), we expect you to provide us with:\n", "<ol>\n", "   <li> a PDF file (written in LaTeX, see example on Moodle) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);\n", "   <li> this Jupyter Notebook (it will not be read, just checked for plagiarism);\n", "   <li> and all other files (not the datasets!) we would need to run your code.\n", "</ol>\n", "</div>\n", "\n", "\n", "\n", "<br><font size=5 color=#009999> <b>CONTEXT & OBJECTIVE </b> </font> <br>\n", "    \n", "\n", "### Context\n", "\n", "The classification scheme of galaxies, quasars, and stars is one of the most fundamental in astronomy. In this hackathon, we will investigate the classification of stars based on their spectral characteristics. The early cataloguing of stars and their distribution in the sky has led to the understanding that they make up our own galaxy and, following the distinction that Andromeda was a separate galaxy to our own, numerous galaxies began to be surveyed as more powerful telescopes were built. This datasat aims to classificate stars, galaxies, and quasars based on their spectral characteristics.\n", "\n", "![](https://storage.googleapis.com/kaggle-datasets-images/1866141/3047436/9aaba4abadbffd3dcdb1e0fab8b75a6d/dataset-cover.jpg?t=2022-01-15-17-18-30)\n", "\n", "\n", "### Objectives\n", "\n", "The data consists of 100,000 observations of space taken by the SDSS (Sloan Digital Sky Survey). Every observation is described by 17 feature columns and 1 class column which identifies it to be either a star, galaxy or quasar. The project aims at building a ternary classifier for the following 3 classes: star, galaxy or quasar. \n", "\n", "\n", "### Notebook structure\n", "This notebook is organized into four parts. Each of them assesses one fundamental step to solve our problem and provides one visualization tool to gain some understanding:\n", "* PART 1 - DATA LOADING\n", "   - 1.1 - Discover the dataset\n", "   - Bonus: Visualization of alpha and delta \n", "   - 1.2 - Discard irrelevant features \n", "   - 1.3 - Split the dataset\n", "    <br><br>\n", "* PART 2 - EXPLORATORY DATA ANALYSIS \n", "   - 2.1 - Ternary targets\n", "   - 2.2 - Correlation matrix\n", "   - 2.3 - Data scaling and normalization\n", "    <br><br>\n", "* PART 3 - MODEL SELECTION\n", "   - 3.1 - Precision, recall and F1-score\n", "   - 3.2 - Model evaluation\n", "   - 3.3 - Model selection and parameters tuning\n", "   - 3.4 - Precision-Recall curve and thresholding\n", "   <br><br>\n", "* PART 4 - MODEL TESTING\n", "   - 4.1 - Error computation on the test set\n"]}, {"cell_type": "code", "execution_count": null, "id": "a56ae85a", "metadata": {}, "outputs": [], "source": ["## warnings off\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")"]}, {"cell_type": "markdown", "id": "b3966f68", "metadata": {}, "source": ["<br><br><font size=7 color=#009999> <b>PART 1 - DATA LOADING</b> </font> <br>\n", "\n", "<font size=4 color=#009999> <br> 1.1 - DISCOVER THE DATASET </font> <br>\n", "\n", "**Import** `star_classification.csv` using `read_csv` [<sup>1</sup>](#fn1) from pandas and **obtain** a brief description of the data (size, variables type, missing values, etc.).  \n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 1.1]</b> Describe, briefly, your dataset (size, variables type, missing values, etc.).<br>\n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "d0b06559", "metadata": {"scrolled": false}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b01 : Import the dataset using pd.read_csv function \n", "\n", "@pre: filename 'star_classification.csv', located in the same folder as this jupyter\n", "@post: variable `df` containing the dataframe\n", "\"\"\"\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "\n"]}, {"cell_type": "markdown", "id": "69b25235", "metadata": {}, "source": ["<font size=4 color=#009999> <br> BONUS - VISUALIZATION OF ALPHA AND DELTA </font> <br>\n", "\n", "The `alpha` and `delta` features encodes the angular locations of the objects. Those locations can be projected on a celestial sphere, showing the distribution of objects in the sky. Purpose of the following cell is for visualization only. \n"]}, {"cell_type": "code", "execution_count": null, "id": "d7eb1389", "metadata": {}, "outputs": [], "source": ["!pip install astropy # Comment this line once the package has been installed\n", "\n", "from astropy.coordinates import SkyCoord\n", "import astropy.units as u\n", "\n", "df_galaxy = df.loc[df['class']=='GALAXY']\n", "df_quasar = df.loc[df['class']=='QSO']\n", "df_star   = df.loc[df['class']=='STAR']\n", "\n", "np.asarray(df_galaxy['alpha'])*u.degree\n", "\n", "coords_galaxy = SkyCoord(ra=np.asarray(df_galaxy['alpha'])*u.degree, dec=df_galaxy['delta']*u.degree, frame='icrs')\n", "coords_quasar = SkyCoord(ra=np.asarray(df_quasar['alpha'])*u.degree, dec=df_quasar['delta']*u.degree, frame='icrs')\n", "coords_star   = SkyCoord(ra=np.asarray(df_star['alpha'])*u.degree, dec=df_star['delta']*u.degree, frame='icrs')\n", "\n", "fig = plt.figure(figsize=(15,15))\n", "ax  = fig.add_subplot(111, projection='mollweide')\n", "ax.scatter(coords_galaxy.ra.wrap_at(180*u.degree).radian, coords_galaxy.dec.radian, \n", "               s=1, label='galaxy', color='limegreen', alpha=0.8)\n", "ax.scatter(coords_quasar.ra.wrap_at(180*u.degree).radian, coords_quasar.dec.radian, \n", "               s=1, label='quasar', color='yellow', alpha=0.8)\n", "ax.scatter(coords_star.ra.wrap_at(180*u.degree).radian, coords_star.dec.radian, \n", "               s=1, label='star', color='steelblue', alpha=0.8)\n", "ax.legend(markerscale=6)\n", "ax.grid()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "36edc319", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 1.2 - DISCARD IRRELEVANT FEATURES </font> <br>\n", "\n", "It is usually hard to determine in adavace which features, or combination of features, will be the most relevant to perform the classification task. The rule of thumb is to **keep all the features** for which you have **doubts**. In some specific cases, however, it is possible to identify certain features that can make no contribution to the classification task. To do this, it is useful to observe the distribution of our observations for each feature. \n", "\n", "We therefore focus on identifying **features whose distribution cannot provide any relevant information** for classification. It is therefore a question of identifying them on the basis of a **numerical decision** and **not** on the basis of a decision linked to the **physical meaning** of the feature.\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 1.2]</b> Based on a study of the features distribution (variance, number of unique values, number of missing values, etc.), can you identify some features that do not provide useful information for the classification task? Explain your analysis and remove those features from the dataset. <br>\n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "f37e027a", "metadata": {"scrolled": true}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b02 : Remove useless features\n", "\n", "@pre: variable `df` containing the dataframe\n", "@post: variable `df` containing the reduced dataframe\n", "\"\"\"\n", "\n"]}, {"cell_type": "markdown", "id": "cd75d132", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 1.3 - SPLIT THE DATASET </font> <br> \n", "\n", "Data science projects begin by the division the **whole** dataset into a **training** and a **test** set. The subsequent analysis and decisions (i.e. features selection, pre-processing, model selection, etc.) are, then, conducted only on the _training set_ to stay statistically significant during the **testing phase**. The latter will, thus, only be conducted on the _test set_.  \n", "\n", "We invite you then to **split** [<sup>2</sup>](#fn2) the dataset into a _training_ and a _test_. The proportion of each subset is at **your own discretion**.\n", "\n", "\n", "<span id=\"fn2\"> [2] N.B. Set the seed of your random split with `random_state = 42` to obtain reproducible results.</span>\n", "\n", "\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 1.3]</b> What are the drawbacks (if any) of choosing a small test set (in proportion)? On the contrary, what are the consequences (if any) of a relatively large testing set (in proportion)? <br>\n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "e8942cce", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b03 : SPLIT THE DATASET    \n", "\n", "@pre:  'df' a pandas frame with the entire dataset\n", "@post: 2 pandas frames with the train and test sets\n", "\"\"\"\n", "\n", "import sklearn.model_selection\n", "\n", "data_train, data_test = ...\n", "\n"]}, {"cell_type": "markdown", "id": "04f7266f", "metadata": {}, "source": ["<br>\n", "\n", "<br><font size=7 color=#009999> <b>PART 2 - EXPLORATORY DATA ANALYSIS </b> </font> <br><br>\n", "\n", "We conduct the analysis on the <i>training set</i>, avoiding therefore any modelling decision based on _unseen_ data (<i>test set</i>). In most cases, we assume that the distribution of this latter set stays similar to the <i>training set</i>.\n", "\n", "<font size=4 color=#009999> <br> 2.1 - TERNARY TARGETS   </font> <br>\n", "\n", "It's always useful to have an idea of the difficulty of the task in hand. More specifically, it is possible to determine a priori the expected **performance of a random classifier**. This constitutes the **baseline** that our model will logically have to beat. \n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 2.1]</b> Are the ternary classes balanced? What are the proportions of data in each class? Briefly, justify your answer and add a visualization.\n", "</div> \n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 2.2]</b> What would be the expected performance of a random classifier on this dataset?\n", "</div> \n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "6b2c988e", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b04 : TERNARY CLASSES :  proportion and ternarization\n", "\n", "@pre:  Training dataframe  \n", "@post: Proportion of each ternary class in this train set, a graph representing it, \n", "a modified training dataframe with a new column 'classes_ternary' continaining the ternary targets (0,1,2)\n", "\"\"\"\n", "\n"]}, {"cell_type": "markdown", "id": "2cfd78bf", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 2.2 - CORRELATION MATRIX </font> <br>\n", "\n", "__Compute__ and __plot__ the correlation matrix. For the plot, you can use the function `imshow` or `matshow` from `matplotlib`.\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 2.3]</b> Compute the correlation matrix of the dataset and plot it. Do you want to discard features based on this observation?  <br>\n", "Write clearly you decision rule.                                                                                      \n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "c4402718", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b05 : CORRELATION MATRIX\n", "   \n", "@pre:  Training dataframe  \n", "@post: The correlation matrix between the features (target incl.) and its plot    \n", "\"\"\"\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "f4f8be4d", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 2.3 - DATA SCALING AND NORMALIZATION</font> <br> \n", "\n", "__Split__ your _training_ and _test_ sets into their respective features set  (X)  and a ternary target variable (y). __Standardize__ the features sets. \n", "\n", "\n", "__Remark 1.__ The scaler object, used to scale the <i>training set</i>, should also be the one used on the <i>test set</i>! Again, do no reinvent the wheel!  \n"]}, {"cell_type": "markdown", "id": "27a11066", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "    <b>[Question 2.4]</b>  Why do we scale data? Justify properly, whether it is necessary or not for your feature set (X) and which scaler did you use.\n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "efccbf19", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b06 : Data fill-in & scaling\n", "   \n", "@pre: train and test dataframes and the list 'feature_names' of columns to keep    \n", "@post:  X_train: numpy array, with (scaled) selected features, containing training data\n", "        X_test: numpy array, with (scaled) selected features, containing testing data. \n", "                The scaling should be done using the statistic of the train set.\n", "\"\"\"\n", "\n", "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n", "\n", "\n", "## Feature to discard (add some if necessary)\n", "feature_names = [\"class\"]\n", "X_train       = data_train.drop(columns = feature_names)\n", "X_test        = data_test.drop(columns = feature_names)\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "60666151", "metadata": {}, "source": ["<br>\n", "\n", "<font size=4 color=#009999>  </font> <br>\n", "\n", "<br><font size=7 color=#009999> <b>PART 3 - MODEL SELECTION </b> </font> <br><br>\n", "\n", "\n", "Let us first build some tools that will help us to choose among our investigated models together with their (hyper-)parameters which one performs the best. "]}, {"cell_type": "markdown", "id": "c43e5884", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 3.1 - PRECISION, RECALL AND F1 SCORE </font> <br>\n", "\n", "**Implement** the _precision, recall_ and _F-measure_ metrics based on the confusion matrix. Please follow the specifications in the provided template.  <br>\n", "\n", "**Reminder**\n", "\n", "$F_1$ is a performance score allowing to obtain some trade-off between the precision and recall criterions. It can be computed as follows:\n", "$$F_1 = 2~\\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}.$$\n", "\n", "Please consult, [Wikipedia](https://en.wikipedia.org/wiki/F-score) for further information about the metric.\n", "\n", "**NOTE**: if ever the model you built would be terribly <i>bad</i>, with both precision's and recall's value equal to $0$, we suggest to return $0$ as $F_1$ score. "]}, {"cell_type": "code", "execution_count": null, "id": "b12daff7", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b07 : Implementation of precision, recall & F1 scores\n", "\n", "@pre:  /  \n", "@post: Follow the specifications to implement precision, recall and probas_to_F1 functions. \n", "\"\"\"\n", "from sklearn.metrics import confusion_matrix\n", "\n", "\"\"\" -----------------------------------------------------------------------------------------\n", "Converts a vector of real probability values to a binary 0 or 1 \n", "@pre: \n", "    - proba_vec: vector with real values representing each a probability\n", "    - threshold : a threshold probability (between 0 and 1)\n", "@post:\n", "    - predicted_labels: binary prediction vector, with elements being 0 or 1.\n", "----------------------------------------------------------------------------------------- \"\"\"\n", "\n", "\n", "def pred_probas_to_pred_labels(proba_vec, threshold=0.5):\n", "    return np.where(proba_vec <= threshold, 0, 1)\n", "\n", "\n", "\"\"\" -----------------------------------------------------------------------------------------\n", "Based on the confusion matrix, computes the 'precision'\n", "@pre: \n", "    - cm : confusion_matrix of a binary classification\n", "@post:\n", "    - score: precision (or positive predictive value), associated with cm\n", "----------------------------------------------------------------------------------------- \"\"\"\n", "\n", "\n", "def precision(cm):\n", "\n", "\n", "    return ppv\n", "\n", "\n", "\"\"\" -----------------------------------------------------------------------------------------\n", "Based on the confusion matrix, computes the 'recall'\n", "@pre: \n", "    - cm : confusion_matrix of a binary classification  \n", "@post:\n", "    - r: recall (or true positive rate), associated with cm\n", "----------------------------------------------------------------------------------------- \"\"\"\n", "\n", "\n", "def recall(cm):\n", "\n", "    return tpr\n", "\n", "\n", "\"\"\" -----------------------------------------------------------------------------------------\n", "Evaluates the F1 score which is a harmonic mean of the precision and recall\n", "@pre: \n", "    - y_true: vectors of 0 and 1 representing the real class values\n", "    - y_pred: vectors of real values representing predicted probability\n", "    - output:  'F1' means that the output should only be the F1 score. \n", "               'PRF1' means that the output is a tuple with (precision, recall, F1)\n", "               'F1' is the default value\n", "    - threshold: a threshold probability (between 0 and 1) \n", "@post:\n", "    - F1_score: harmonic mean of the precision and recall\n", "    - If asked in argument, precision and recall can be added in the output: (precision, recall, F1)\n", "----------------------------------------------------------------------------------------- \"\"\"\n", "\n", "\n", "def probas_to_F1(y_true, y_pred, output=\"F1\", threshold=0.5):\n", "\n", "    y_pred = pred_probas_to_pred_labels(y_pred, threshold)\n", "\n", "\n", "    return F1_score"]}, {"cell_type": "markdown", "id": "bd625b5b", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 3.2 - MODEL EVALUATION  </font> <br>\n", "\n", "**Implement** `evalParam`, which evaluates, using a __k-fold__ cross-validation, a list of `scikit-learn` models. Use your method `probas_to_F1` as score function. The function `evalParam` must be  **scalable**. Put differently, it must handle $m$ methods, and a variable list of their possible parameters configuration. \n", "\n", "\n", "In addition of the list of _models_ (methods) and their list of _hyperparameters_ (param), the function takes as arguments the _features set_ (X), _target variable_ (y) and _the number of folds_ (cv). \n", "\n", "It returns an array _score_ such that <br>\n", "\n", "$$score[i][j] = average F1 over the folds, using method _i_ with parameters configuration j.$$\n", "\n", "To help you, here is a pseudo code of K-fold for one method and one configuration of hyperparameters. \n", "\n", "<img src=\"K-fold_pseudo-code.png\" width = \"650\">\n", " \n", "__Remark 1.__ You have to implement a K-fold cross-validation. You are only allowed to use `KFold.splits(dataset)` from `sklearn.model_selection` to generate the indices of your different folds. \n"]}, {"cell_type": "markdown", "id": "3b3deab6", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "    <b>[Question 3.1]</b>\n", "    Explain the idea of K-fold cross-validation and why it is useful. How the choice of K (in the cross-validation) impacts the bias and the variance of the scores obtained on the different folds? Choose and justify the number of folds you consider in this project. \n", "</div> "]}, {"cell_type": "code", "execution_count": null, "id": "4e2dc578", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b08 : Evaluates the methods using different parameters via a K-folds with cv folds\n", "\n", "@pre: \n", "    - methods: list of classifiers to analyze\n", "    - param: list of size len(methods) containing lists of parameters (in dictionary form) to evaluate.\n", "             In other words, param[i][j] is a dictionary of parameters.\n", "             For example if index i is for KNN, we can have a parameter configuration (with index j) described as\n", "                 param[i][j] = {\"n_neigbors\":5, \"weights\": 'uniform'}; \n", "                 while param[i] is a list of such parameters dictionnaries for model i (here KNN)\n", "    - X: training dataset\n", "    - y: target vector for the corresponding entries of X\n", "    - cv: the number of folds to use in your cross-validation\n", "@post:\n", "    - score: list with same shape as param. score[i][j] = mean score over the folds, \n", "                                                         using method i with parameters param[i][j]\n", "------------------------------------------------------------------------------------------------ \"\"\"\n", "from sklearn.model_selection import KFold\n", "\n", "\n", "def evalParam(methods, param, X, y, cv):\n", "\n", "    return score"]}, {"cell_type": "markdown", "id": "b1f91ea2", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 3.3 - MODEL SELECTION AND PARAMETERS TUNING </font> <br>\n", "\n", "__Run__ your function `evalParam` to evaluate the three following models : [_linear regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), [_logistic regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [_K nearest neighbors_](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). <br>These models are already implemented in sklearn. <br>\n", "\n", "**Study** the effect of the following hyperparameters:\n", "- `n_neighbors` in KNN (try selected values between 1 and 100),\n", "- `weights` in KNN (try both values 'uniform' and 'distance')\n", "- `p` in KNN (try euclidean (p=2), manhattan (p=1) and minkowski-100 (more or less equivalent to max-norm))\n", "- `C` in logistic regression (try selected values between $10^{-3}$ and $10^3$)</li>\n", "\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question you should ask yourself]</b> (Not graded) Prior to the run, discuss the fitness of each model to answer to our problem. \n", "</div> \n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 3.2]</b> Explain your methodology of model evaluation. More precisely, explain which hyperparameters you tune and the values you test for each of them. Next, provide the best hyperparameters configuration for each of the three models as well as their CV F1 score.\n", "</div>\n"]}, {"cell_type": "code", "execution_count": null, "id": "dbb70230", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b09 : Model selection - tuning the three methods\n", "   \n", "@pre: evalParam function correctly implemented    \n", "@post:  three models (knn, linear and logistic regression) initialized with tuned hyperparameters.\n", "        print the best hyperparameters found, as well as their CV F1 scores associated with these hyperparameters.\n", "------------------------------------------------------------------------------------------------ \"\"\"\n", "\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "\n", "print(' ')\n", "\n", "\n"]}, {"cell_type": "markdown", "id": "eae36d47", "metadata": {}, "source": ["<div class=\"alert alert-warning\">\n", "    <b>[Question 3.3]</b> Based on your answers to previous questions, select a final model that you will keep as classifier. Justify.\n", "</div> "]}, {"cell_type": "markdown", "id": "3f58c673", "metadata": {}, "source": ["<font size=4 color=#009999> <br> 3.4 - PRECISION, RECALL AND THRESHOLDING  </font> <br>\n", "\n", "In general, the classifying models compute first the **probability** for a point to belong to a certain class. Next, they apply a **threshold** to assign the final label (star, galaxy or quasar). By default, `scikit-learn` applies a threshold of 0.5 for KNN and logistic regression. You can use the function `predict_proba` to obtain the original probabilities. <br>\n", "For analyzing the impact of the threshold on the precision and recall of a model on binary classification, we generally plot its **precision-recall curve**. Specific functions in sklearn help doing that plot (see [sklearn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)). <br>\n", "\n", "In order to extend the precision-recall curve our **multi-class** problem, it is necessary to binarize the output. For example, for class `star`, the binarization of the output `(galaxy, star, star, quasar, galaxy, star, quasar)` is `(0,1,1,0,0,1,0)`. We then consider the classification of the `star` vs all other classes. Then, we repeat the process to produce the classification of `galaxy` vs all and `quasar` vs all. \n", "For each of the three classes: binarize the output, train your model on the binarized output, make a binary prediction and plot its precision-recall curve for the three methods, on the same figure. There should be one figure for each class, with the precision-recall curve for the three methods on each figure.<br>\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 3.4]</b> Plot the precision-recall curve for the three methods, one figure for each class. What happens to the precision and recall when the threshold tends to 0? And when it tends to 1? Explain and, if possible, establish a link with Question 2.1.<br>\n", "    For each class, for each method: what threshold would you use?\n", "</div>\n"]}, {"cell_type": "code", "execution_count": null, "id": "aa8cf2da", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "TEST CELL N\u00b010 : PRECISION-RECALL CURVES\n", "   \n", "@pre: the three models (knn, linear and logistic regression) initialized with their tuned hyperparameters.  \n", "@post: a figure with the precision-recall curves for the three given models and also for a simple baseline classifier, \n", "        applied on a validation set containing 10% of the training set.\n", "------------------------------------------------------------------------------------------------ \"\"\"\n", "\n", "from sklearn.metrics import precision_recall_curve\n", "from sklearn.metrics import auc\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import label_binarize\n", "\n", "# Validation set to use for the PR curves\n", "X_train2, X_val = train_test_split(X_train, test_size=0.1, random_state=42)\n", "y_train2, y_val = train_test_split(y_ternary_train, test_size=0.1, random_state=42)\n", "\n", "# Binarization of the output\n", "y_train2_bin = label_binarize(y_train2, classes=[0, 1, 2])\n", "y_val_bin    = label_binarize(y_val, classes=[0, 1, 2])\n", "n_classes    = y_val_bin.shape[1]\n", "\n"]}, {"cell_type": "markdown", "id": "9d72674d", "metadata": {}, "source": ["<br>\n", "\n", "<br><font size=7 color=#009999> <b>PART 4 - MODEL TESTING </b> </font> <br><br>\n", "\n", "<div class=\"alert alert-warning\">\n", "    <b>[Question 4.1]</b> Use the test set to estimate the precision, recall and F1 score of your final model and validate its performance on unseen data. <br> Observe if the scores are similar to the ones estimated with your cross-validation.\n", "        Are you satisfied by the performance of your classifier, in view of the task for which it will be used?\n", "\n", "</div> \n"]}, {"cell_type": "code", "execution_count": null, "id": "d90ddd75", "metadata": {}, "outputs": [], "source": ["\"\"\"\n", "CELL N\u00b011 : MODEL TESTING\n", "   \n", "@pre:   clf is your selected classifier\n", "        X_test is the numpy array containing the test set (with your selected features)\n", "        y_test is the numpy array contaning your binary target vector\n", "@post:  print the F1, precision and recall on the test set.\n", "------------------------------------------------------------------------------------------------ \"\"\"\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "17f1ab1a", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.12"}}, "nbformat": 4, "nbformat_minor": 5}