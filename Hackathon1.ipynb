{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon Stat\n",
    "\n",
    "This project illustrates the course LEPL1109 with an industrial application of statistics. You will analyse the capacity of solar production of electricity located in the French cities of Caen and Tours.\n",
    "The file 'radiation.csv' contains 3 columns \n",
    "DATE           : YYYYMMDD,\n",
    "Caen and Tours : the daily solar radiation in W/m2 measured in the 2 cities. \n",
    "Notice that data for some days are not reported due to failure of measurement system.\n",
    "\n",
    "## Report content\n",
    "\n",
    "•\tYou have to fill in this  jupyter notebook downloadable on the moodle website of the course\n",
    "\n",
    "•\tGrades are granted to the members whose names are in the Jupyter notebook. If your name doesn’t appear on the top of the notebook, you’ll get a 0, even though you are in a group on Moodle.\n",
    "\n",
    "•\tThe jupyter notebook must be compiled with printed results and next submitted via moodle. The absence of compiled results (or non-printed values) leads to a lower grade.\n",
    "\n",
    "## Report submission\n",
    "\n",
    "•\tThe deadline for submission is reported on the moodle website. Submission after the deadline will not be accepted.\n",
    "\n",
    "•\tTo submit your report, go to the section “APP” on Moodle and the subsection “Soumission du rapport”. You can upload your work there. Once you are sure that it is your final version, click the button “Envoyer le devoir”. It is important that you don’t forget to click on this button ! \n",
    "\n",
    "•\tReports that have not been uploaded through Moodle will not be corrected.\n",
    "\n",
    "## Names and Noma of participants:\n",
    "\n",
    "Part. 1: Lebras Floriane         (35022100)\n",
    "\n",
    "Part. 2: Martin Antoine           (86692100)\n",
    "\n",
    "Part. 3: Dallemagne Brieuc        (77122100)\n",
    "\n",
    "Part. 4: De Vleeschouwver Nora    (48602100)\n",
    "\n",
    "Part. 5: Debelle Thomas           (30002100)\n",
    "\n",
    "Part. 6: Orékhoff Alexandre       (54552100)\n",
    "\n",
    "---\n",
    "## 1. Energy calculation and basic statistics\n",
    "\n",
    "Compute the daily energy in WH per square meter of solar panel. For this purpose you use the datasets reporting the solar irradation measure in Tours and Tours (source https://www.ecad.eu/). The irradiation is measured in W/m2 per day. You will use the formula:\n",
    "\n",
    "C = E_Sol x 24 x P_cr x f_perf\n",
    "\n",
    "where  \n",
    "\n",
    "C is the electricity produced in WH/m2 for a day\n",
    "\n",
    "E_sol is the daily solar radiation in W/m2 \n",
    "\n",
    "P_cr is the peak power coefficient, set here to  0.18 (monocristal silicium)\n",
    "\n",
    "f_perf depends upon the system, set here to 0.75.\n",
    "\n",
    "Remark:\n",
    "\n",
    "1 W = 1 J/sec\n",
    "\n",
    "1 WH  is 1W x 3600sec = 3600J\n",
    "\n",
    "energy/m2 = E_sol * 24 * 3600 J/m2 = E_sol * 24 WH/m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1.1. Start by computing the daily energy in WH produced by a 1m2 solar panel\n",
    "\n",
    "a. Plot time-series of solar electric production in Caen and Tours from 1974 to 2023. Comment the evolution.\n",
    "\n",
    "b. Plot boxplots of daily productions for both cities. Comment the box plot.\n",
    "\n",
    "c. Remove outliers using the interquartile range. \n",
    "\n",
    "d. Plot an histogram of daily electricity production, after removal of outliers.\n",
    "\n",
    "Watchout: remove all days for which a outlier is observed in Caen **or** Tours to keep the same number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prérequis Import ###\n",
    "\n",
    "import csv\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv(\"Radiation.csv\")\n",
    "data_pd = data_pd.drop(columns=\"Unnamed: 0\")\n",
    "Caen = data_pd[\"Caen\"].to_numpy()\n",
    "Tours = data_pd[\"Tours\"].to_numpy()\n",
    "Date = data_pd[\"DATE\"].to_numpy()\n",
    "Index = np.arange(0, len(Date), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code 1.1 ###\n",
    "\n",
    "def time_to_year(dataset, month):\n",
    "    dataset = dataset.copy()\n",
    "    dataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"], format=\"%Y%m%d\")\n",
    "    \n",
    "    dataset[\"Year\"] = dataset[\"DATE\"].dt.year\n",
    "    dataset[\"Month\"] = dataset[\"DATE\"].dt.month\n",
    "    dataset = dataset[dataset[\"Month\"] == month]\n",
    "    dataset = dataset.drop(columns=[\"Year\", \"Month\"])    \n",
    "    \n",
    "    dataset[\"DATE\"] = dataset[\"DATE\"].dt.strftime(\"%Y\") ## Make a copy if you want to keep the most interesting data\n",
    "    return dataset\n",
    "\n",
    "def time_to_month(dataset):\n",
    "    dataset = dataset.copy()\n",
    "    dataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"], format=\"%Y%m%d\")\n",
    "    dataset[\"DATE\"] = dataset[\"DATE\"].dt.strftime(\"%m\") ## Make a copy if you want to keep the most interesting data\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def box_plot(data1, data2, title, name1, name2):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot a box plot of two data\n",
    "\n",
    "    args:\n",
    "        data1: list of data\n",
    "        data2: list of data\n",
    "        title: title of the plot\n",
    "        name1: name of the first data\n",
    "        name2: name of the second data\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    bp = plt.boxplot([data1, data2], patch_artist=True, labels=[name1, name2])\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen']\n",
    "    for box, color in zip(bp['boxes'], colors):\n",
    "        box.set(facecolor=color)\n",
    "    \n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='o', markersize=5, markerfacecolor='red')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data(data,title,Xname,Yname):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot a data\n",
    "    \n",
    "    args:\n",
    "        data: list of data\n",
    "        title: title of the plot\n",
    "        Xname: name of the X axis\n",
    "        Yname: name of the Y axis\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(Xname)\n",
    "    plt.ylabel(Yname)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def remove_outliers_interquartile(lst1,lst2,Date,Index):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        remove outliers from two lists of data by using the interquartile method\n",
    "\n",
    "    args:\n",
    "        lst1: np.array of data1\n",
    "        lst2: np.array of data2\n",
    "\n",
    "    return:\n",
    "        new_lst1: list of data1 without outliers\n",
    "        new_lst2: list of data2 without outliers\n",
    "        new_date: list of dates without outliers\n",
    "        new_index: list of indexes of the data without outliers\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(lst1, 25)\n",
    "    q3 = np.percentile(lst1, 75)\n",
    "    iqr = q3 - q1\n",
    "    Q1 = np.percentile(lst2,25)\n",
    "    Q3 = np.percentile(lst2,75)\n",
    "    IQR = Q3 - Q1\n",
    "    new_lst1 = list()\n",
    "    new_lst2 = list()\n",
    "    new_date = list()\n",
    "    new_index = list()\n",
    "    for i in range(len(lst1)):\n",
    "        if lst1[i] > q1 - 1.5*iqr and lst1[i] < q3 + 1.5*iqr and lst2[i] > Q1 - 1.5*IQR and lst2[i] < Q3 + 1.5*IQR:\n",
    "            new_lst1.append(lst1[i])\n",
    "            new_lst2.append(lst2[i])\n",
    "            new_date.append(Date[i])\n",
    "            new_index.append(Index[i])\n",
    "    return [new_lst1,new_lst2,new_date,new_index]\n",
    "\n",
    "def plot_histogram(data, title, x_label, y_label, color='skyblue'):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot an histogram of a data\n",
    "    \n",
    "    args:\n",
    "        data: list of data\n",
    "        title: title of the plot\n",
    "        x_label: name of the X axis\n",
    "        y_label: name of the Y axis\n",
    "        color: color of the histogram\n",
    "    \n",
    "    Return: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(data, color=color, edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "P_cr = 0.18\n",
    "f_perf = 0.75\n",
    "E_caen = Caen\n",
    "E_Tour = Tours\n",
    "C_caen = Caen*24*P_cr*f_perf\n",
    "C_Tour = Tours*24*P_cr*f_perf\n",
    "\n",
    "\n",
    "plot_data(C_caen,'C_caen','Jour','Electricité(W/m2)')\n",
    "plot_data(C_Tour,'C_Tour','Jour','Electricité(W/m2)')\n",
    "\n",
    "box_plot(C_caen,C_Tour,'C_caen et C_Tour','C_caen','C_Tour')\n",
    "\n",
    "res = remove_outliers_interquartile(C_caen,C_Tour,Date,Index)\n",
    "C_caen = res[0]\n",
    "C_Tour = res[1]\n",
    "Date = res[2]\n",
    "Index = res[3]\n",
    "\n",
    "plot_histogram(C_caen,'C_caen','Jour','Electricité(W/m2)')\n",
    "plot_histogram(C_Tour,'C_Tour','Jour','Electricité(W/m2)',color='lightgreen')\n",
    "\n",
    "\"\"\"\n",
    "commentaire 1.1.a: sur les deux premiers plots on peut voir que à part certaines valeurs grandement supérieurs vers la fin,\n",
    "les données des deux villes restent dans une range entre 0 et 1500.\n",
    "\n",
    "commentaire 1.1.b: les plots de boxplots nous confirment que la plupart valeurs sont dans une range entre 0 et 1500. \n",
    "On peut voir que du côté de Tour la range est un peu plus grande que celle de Caen. On peut aussi voir que la médiane\n",
    "de Tour est un tout petit peu plus grande que celle de Caen.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1.2. We want to compute monthly statistics of electricity solar production. Calculate for each city and for each month: \n",
    "\n",
    "1) the average daily production of electricity in Wh/m2\n",
    "\n",
    "2) the median daily production of electricity in Wh/m2\n",
    "\n",
    "3) the standard deviation daily production of electricity in Wh/m2\n",
    "\n",
    "4) the 5% percentile of daily production of electricity in Wh/m2\n",
    "\n",
    "5) the 95% percentile of daily production of electricity in Wh/m2\n",
    "\n",
    "Report the results in one or two tables. \n",
    "\n",
    "Compare and comment these statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code 1.2 ###\n",
    "\n",
    "### Refaire avec les mois\n",
    "\n",
    "def q2_month(dataset, month):\n",
    "    \"\"\"Code that answer the question 1.2\n",
    "    description:\n",
    "        Compute the average, median, standard deviation, 5 percentile and 95 percentile of a dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (Arraylist): An arraylist with the data of the daily production of electricity\n",
    "\n",
    "    Returns:\n",
    "        tuple: Return a tuple of data like: (average, median, standard deviation, 5 percentile, 95 percentile)\n",
    "    \"\"\"\n",
    "    dataset_av = np.sum(dataset) / len(dataset) # Average / mean\n",
    "\n",
    "    ### Comput median\n",
    "    dataset_median = np.median(dataset)\n",
    "\n",
    "    dataset_std = np.std(dataset)\n",
    "\n",
    "    dataset_5 = np.percentile(dataset, 5)\n",
    "    dataset_95 = np.percentile(dataset, 95)\n",
    "        \n",
    "    print(f\"{month}:  {dataset_av:.5f}\\t{dataset_median:.5f}\\t{dataset_std:.5f}\\t{dataset_5:.2f}\\t{dataset_95:.2f}\")\n",
    "    \n",
    "    return (dataset_av, dataset_median, dataset_std, dataset_5, dataset_95)\n",
    "\n",
    "def q2(dataset):\n",
    "    \"\"\"Answer Q2\n",
    "\n",
    "    Args:\n",
    "        dataset (pandas dataframe): a data frame with the data column being the month\n",
    "    \"\"\"\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "    cities = [\"Caen\", \"Tours\"]\n",
    "    dataset = dataset.sort_values(by=[\"DATE\"])\n",
    "    \n",
    "    #print(dataset)\n",
    "    \n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        print(\"Month: Average  Median \\t\\tStandard dev \\t5 quar  95 quartile\")\n",
    "        for month in months:\n",
    "            q2_month(dataset[dataset[\"DATE\"] == month][city].to_numpy(), month)\n",
    "        print(\"_______________\")\n",
    "\n",
    "clean_pd = pd.DataFrame({\"DATE\": Date,\"Caen\": C_caen, \"Tours\": C_Tour})\n",
    "month_pd = time_to_month(clean_pd.copy())\n",
    "q2(month_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "We can see how the power production drastically increase during summer (eg: June and July) and in the opposite dropping during winter.\n",
    "However there is a strong fluctuation in power production (standard deviation). It's normal since a bright sunny day produces way more than a grey and rainy day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fit of distributions and hypothesis tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.1. We focus on the daily production of electricity in April. Retrieve the data for month of April, in Caen and Tours. \n",
    "\n",
    " 1) Fit Gamma and normal distributions by log-likelihood maximization to \n",
    "    daily production of electricity during April (Caen & Tours).\n",
    "    \n",
    " 2) Compute the 4 log-likelihoods and select the best model for each location (justify your answer).\n",
    " \n",
    " 3) Compare on the same plot the empirical, the  gamma and normal pdf (the\n",
    "    empirical pdf is an histogram of frequencies).\n",
    "    \n",
    " 4) Why is there 3 parameters in python for the Gamma pdf whereas there\n",
    "    is only 2 in the distribution seen during lectures? \n",
    "\n",
    "Remark : set floc to -0.001 for the gamma.fit (to avoid troubles in case of null observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "april_pd = clean_pd.copy()\n",
    "april_pd[\"DATE\"] = pd.to_datetime(april_pd[\"DATE\"], format=\"%Y%m%d\")\n",
    "april_pd[\"DATE\"] = april_pd[\"DATE\"].dt.strftime(\"%m\")\n",
    "april_pd = april_pd.loc[april_pd[\"DATE\"] == \"04\"]\n",
    "\n",
    "april_Caen, april_Tours = april_pd[\"Caen\"].to_numpy(), april_pd[\"Tours\"].to_numpy()\n",
    "april_C_Caen = april_Caen*24*P_cr*f_perf\n",
    "april_C_Tours = april_Tours*24*P_cr*f_perf\n",
    "april_C_Caen, april_C_Tours, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "print(april_C_Caen)\n",
    "print(april_C_Tours)\n",
    "\n",
    "norm_fit_Caen_loc, norm_fit_Caen_scale = sp.stats.norm.fit(april_C_Caen)\n",
    "norm_fit_Tours_loc, norm_fit_Tours_scale = sp.stats.norm.fit(april_C_Tours)\n",
    "x_norm_Caen  = np.linspace(sp.stats.norm.ppf(0.01,loc=norm_fit_Caen_loc,scale=norm_fit_Caen_scale), sp.stats.norm.ppf(0.99,loc=norm_fit_Caen_loc,scale=norm_fit_Caen_scale), 100)\n",
    "x_norm_Tours = np.linspace(sp.stats.norm.ppf(0.01,loc=norm_fit_Tours_loc,scale=norm_fit_Tours_scale), sp.stats.norm.ppf(0.99,loc=norm_fit_Tours_loc,scale=norm_fit_Tours_scale), 100)\n",
    "norm_pdf_Caen = sp.stats.norm.pdf(x_norm_Caen,norm_fit_Caen_loc,norm_fit_Caen_scale)\n",
    "norm_pdf_Tours = sp.stats.norm.pdf(x_norm_Tours,norm_fit_Tours_loc,norm_fit_Tours_scale)\n",
    "normal_log_likelihood_maximisation_Caen = np.sum(np.log(norm_pdf_Caen))\n",
    "normal_log_likelihood_maximisation_Tours = np.sum(np.log(norm_pdf_Tours))\n",
    "print(normal_log_likelihood_maximisation_Caen)\n",
    "print(normal_log_likelihood_maximisation_Tours)\n",
    "\n",
    "\n",
    "gamma_fit_Caen_shape, gamma_fit_Caen_loc, gamma_fit_Caen_scale = sp.stats.gamma.fit(april_C_Caen,floc=-0.001)\n",
    "gamma_fit_Tours_shape, gamma_fit_Tours_loc, gamma_fit_Tours_scale = sp.stats.gamma.fit(april_C_Tours,floc=-0.001)\n",
    "x_gamma_Caen  = np.linspace(sp.stats.gamma.ppf(0.01,a= gamma_fit_Caen_shape,loc=gamma_fit_Caen_loc,scale=gamma_fit_Caen_scale), sp.stats.gamma.ppf(0.99,a=gamma_fit_Caen_shape,loc=gamma_fit_Caen_loc,scale=gamma_fit_Caen_scale), 100)\n",
    "x_gamma_Tours = np.linspace(sp.stats.gamma.ppf(0.01,a= gamma_fit_Tours_shape,loc=gamma_fit_Tours_loc,scale=gamma_fit_Tours_scale), sp.stats.gamma.ppf(0.99,a=gamma_fit_Tours_shape,loc=gamma_fit_Tours_loc,scale=gamma_fit_Tours_scale), 100)\n",
    "gamma_pdf_Caen = sp.stats.gamma.pdf(x_gamma_Caen,gamma_fit_Caen_shape,gamma_fit_Caen_loc,gamma_fit_Caen_scale)\n",
    "gamma_pdf_Tours = sp.stats.gamma.pdf(x_gamma_Tours,gamma_fit_Tours_shape,gamma_fit_Tours_loc,gamma_fit_Tours_scale)\n",
    "gamma_log_likelihood_maximisation_Caen = np.sum(np.log(gamma_pdf_Caen))\n",
    "gamma_log_likelihood_maximisation_Tours = np.sum(np.log(gamma_pdf_Tours))\n",
    "print(gamma_log_likelihood_maximisation_Caen)\n",
    "print(gamma_log_likelihood_maximisation_Tours)\n",
    "\n",
    "if normal_log_likelihood_maximisation_Caen > gamma_log_likelihood_maximisation_Caen:\n",
    "    log_likelihood_maximisation_Caen = normal_log_likelihood_maximisation_Caen\n",
    "else:\n",
    "    log_likelihood_maximisation_Caen = gamma_log_likelihood_maximisation_Caen\n",
    "if normal_log_likelihood_maximisation_Tours > gamma_log_likelihood_maximisation_Tours:\n",
    "    log_likelihood_maximisation_Tours = normal_log_likelihood_maximisation_Tours\n",
    "else:\n",
    "    log_likelihood_maximisation_Tours = gamma_log_likelihood_maximisation_Tours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2:\n",
    "\n",
    "The distribution model with a bigger log likelihood maximisation is the better one. (-> check why)\n",
    "\n",
    "==> The bigger is the log likelihood maximisation, the closer we are to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Empirical, Gamma and Normal pdfs of the daily production of electricity in April in Caen')\n",
    "ax2.set_title('Empirical, Gamma and Normal pdfs of the daily production of electricity in April in Tours')\n",
    "ax1.set_xlabel(r'Daily production of electricity $[WH/m^2]$')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax2.set_xlabel('Daily production of electricity $[WH/m^2]$')\n",
    "ax2.set_ylabel('Probability')\n",
    "n, bins, patches = ax1.hist(april_C_Caen, 30, density=1,facecolor='skyblue', edgecolor='black', alpha=0.6, label='Empirical pdf Caen')\n",
    "n, bins, patches = ax2.hist(april_C_Tours, 30, density=1,facecolor='lightgreen', edgecolor='black', alpha=0.6, label='Empirical pdf Tours')\n",
    "ax1.plot(x_gamma_Caen, gamma_pdf_Caen,'r-', lw=2, alpha=0.6, label='Gamma pdf Caen')\n",
    "ax2.plot(x_gamma_Tours, gamma_pdf_Tours,'teal', lw=2, alpha=0.6, label='Gamma pdf Tours')\n",
    "ax1.plot(x_norm_Caen, norm_pdf_Caen,'k-', lw=2, alpha=0.6, label='Normal pdf Caen')\n",
    "ax2.plot(x_norm_Tours, norm_pdf_Tours,'coral', lw=2, alpha=0.6, label='Normal pdf Tours')\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.4:\n",
    "\n",
    "(to check)\n",
    "\n",
    "(3 parameters = a,loc,scale ? compared to alpha, beta in lectures ?)\n",
    "\n",
    "a==>shape of the distribution = alpha in lectures\n",
    "\n",
    "loc==>location (mean?)\n",
    "\n",
    "scale==> 1/beta in lectures\n",
    "\n",
    "Paramètre 'loc' en plus --> shifts the distribution around loc. Without loc and scale specified (loc=0 and scale=1 by default), the pdf is in the standardized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.2. Check if the average daily production in April is the same in Caen and Tours. Let us recall that the null hypothesis is\n",
    "\n",
    "$H_0$: $\\mu_{Caen} = \\mu_{Tours}$.\n",
    "\n",
    "Take care to comment your conclusions. Are all assumptions required to perform this test sastisfied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "\n",
    "\n",
    "\n",
    "Ttest=sp.stats.ttest_ind(X,Y)\n",
    "#I check if the pvalue(=smallest level of significance for which \n",
    "# the data indicate rejection of the null hypothesis) is bigger then alpha to keep H0\n",
    "#I choose alpha=5%\n",
    "if Ttest.pvalue<0.05:\n",
    "    print(\"H0 is rejected\")\n",
    "else:\n",
    "    print(\"H0 is not rejected\")\n",
    "\n",
    "#Having the same variance is required to perform this test and the populations need \n",
    "# to have a normal distribution and be i.i.d. . As we see in the first part of question 2, the normal \n",
    "# distribution is the best when we do a log likelihood maximisation. I test the equality \n",
    "# of variance at the next question and we can see that H0 is rejected so the variance are not the same.\n",
    "#H0 is rejected which means that the average daily production is not the same in Caen and Tours but the test is not valid since we don't respect the assumptions. That's why we have to use the Wilcoxon's test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.3. Test the equality of variance of daily production in April at Caen & Tours?\n",
    "$H_0$: $\\sigma_{Caen}=\\sigma_{Tours}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "\n",
    "n=len(X)\n",
    "S1    = np.std(X,ddof=1) \n",
    "S2    = np.std(Y,ddof=1)\n",
    "Tx     =S1**2/S2**2\n",
    "# I choose alpha=5%\n",
    "alpha = 0.05\n",
    "pval = sp.stats.f.cdf(Tx,dfn=n-1 , dfd=n-1)\n",
    "if Ttest.pvalue<alpha:\n",
    "    print(\"H0 is rejected\")\n",
    "else:\n",
    "    print(\"H0 is not rejected\")\n",
    "\n",
    "#H0 is rejected which means that the variance is not the same at Caen and Tours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.4. Explain the Wilcoxon's test. What is the main advantage of this test compared to the Student's T test. Why is this useful in our project? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A résumer & check)\n",
    "\n",
    "The Wilcoxon signed-rank test !tests the null hypothesis! that two related paired samples come from the same distribution. In particular, it tests whether the distribution of the differences x - y is symmetric about zero. It is a !non-parametric! version of the paired T-test. (source : documentation de scipy.stats.wilcoxon)\n",
    "\n",
    "!Nonparametric statistics is the type of statistics that is not restricted by assumptions concerning the nature of the population from which a sample is drawn. This is opposed to parametric statistics, for which a problem is restricted a priori by assumptions concerning the specific distribution of the population (such as the normal distribution) and parameters (such the mean or variance).! Nonparametric statistics is based on either not assuming a particular distribution or having a distribution specified but with the distribution's parameters not specified in advance (though a parameter may be generated by the data, such as the median). Nonparametric statistics can be used for descriptive statistics or statistical inference. Nonparametric tests are often used when the assumptions of parametric tests are evidently violated. (source : wikipedia)\n",
    "\n",
    "The Wilcoxon test can be a good alternative to the Student's t-test when population means are not of interest; for example, when one wishes to test whether a population's median is nonzero, or whether there is a better than 50% chance that a sample from one population is greater than a sample from another population. (source : wikipedia)\n",
    "\n",
    "A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. It is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. A two-sample location test of the null hypothesis such that the means of two populations are equal. All such tests are usually called Student's t-tests, though strictly speaking that name should only be used if the variances of the two populations are also assumed to be equal (source : wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.5. Apply the Wilcoxon test to distributions of daily productions in April, at Caen and Tours.  What can you conclude about the means of daily production in these 2 cities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "d=np.subtract(april_C_Caen,april_C_Tours)\n",
    "print(d)\n",
    "wilcoxon_test = sp.stats.wilcoxon(d)\n",
    "print(wilcoxon_test.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value very small ==> H0(about means) very unlikely ==> means of daily production in the 2 cities are different (? -> to check) ==>OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Regression and forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.1. Do we observe any trend in the yearly solar production of electricity over the considered period?\n",
    "To answer this question: \n",
    "\n",
    "a. You will compute the average daily production (Wh/m2) during April from 1977 up to 2019 (included).\n",
    "\n",
    "b. You get a time-series of 43 values for each city. Regress these values on the explanatory\n",
    "variables X=(Year-1977). Don't forget to add a constant term and analyze results. \n",
    "\n",
    "c. Plot on the same graph, the predicted and the observed values.\n",
    "\n",
    "d. Comment your results! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New code because the previous one was producing an error\n",
    "year_data_pd = time_to_year(clean_pd, 4)           # Cast the data to year date \n",
    "Caen_av = [0]; Tours_av = [0]                       # Create two empty list for average\n",
    "years = np.arange(1977,2020); cities = [\"Tours\", \"Caen\"]\n",
    "Caen_av = []; Tours_av = []\n",
    "\n",
    "for year in years:\n",
    "    for city in cities:\n",
    "        num = 0\n",
    "        val = 0\n",
    "        for i in year_data_pd[year_data_pd[\"DATE\"] == str(year)][city]:\n",
    "            val += i\n",
    "            num += 1\n",
    "        if city == \"Tours\":\n",
    "            Tours_av.append(val/num)\n",
    "        else:\n",
    "            Caen_av.append(val/num)\n",
    "\n",
    "### Finish Average\n",
    "\n",
    "### B computing the linear regression X = (Year-1977)\n",
    "\n",
    "### Y = A(Year-1977) + C\n",
    "\n",
    "X = np.arange(0, len(years))\n",
    "\n",
    "coef_Caen = np.polyfit(X, Caen_av, 1)\n",
    "coef_Tours = np.polyfit(X, Tours_av, 1)\n",
    "\n",
    "fig, axe = plt.subplots()\n",
    "axe.plot(X, Caen_av, \"o\", color=\"green\", label=\"Consumption in Caen\", alpha=0.3)\n",
    "axe.plot(X, Tours_av, \"o\", color=\"blue\", label=\"Consumption in Tours\", alpha=0.3)\n",
    "axe.set_xlabel(\"Year since 1977\")\n",
    "axe.set_ylabel(r\"Average Daily Consumption [ $ \\frac{Wh}{m^2}$ ]\")\n",
    "axe.plot(X, coef_Caen[0]*X + coef_Caen[1], color=\"green\", label=\"Linear Regression for Caen\", linewidth=3)\n",
    "axe.plot(X, coef_Tours[0]*X + coef_Tours[1], color=\"blue\", label=\"Linear Regression for Tours\", linewidth=3)\n",
    "axe.grid()\n",
    "axe.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "We can see for both cities a certain rising trend. It is more subtle for *Tours* but can be clearly seen for *Caen*. We can link this raise of Average Daily Consumption with the electrification of our society. We can also explain this due to the importance of computers and digital in our society that requires more electricity. This growing market leads to more electrical consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.2. You want to design a model to forecast the solar electric production for the next day (location Caen only). You will work with data over the period 1977 to 2019. \n",
    "\n",
    "Let us denote by C(t) the production on day 't'. The model that we want to fit is called autoregressive and is defined as follows:\n",
    "\n",
    "$$C(t) = \\sum_{k=1}^{10} a_k C(t-k) $$\n",
    "\n",
    "This model is common in time-series analysis and predicts the production of the next day with the  recent observations.\n",
    "\n",
    "a. Split the dataset into a training set (1977 to 2010 included) and a validation set (2011 to 2019 included).\n",
    "\n",
    "b.\tEstimate this model with statsmodels on the training set. \n",
    "\n",
    "c.\tHow would you judge the quality of the predictive model? (Analyze statistics reported by statsmodel)\n",
    "\n",
    "d.\tCompute the Mean Absolute Error (MAE) between predicted and real consumptions (on the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "No need to re-import data in jupyter or to re-do already existing variable \n",
    "\"\"\"\n",
    "\n",
    "### answer to a\n",
    "start_train = (year_data_pd[\"DATE\"] == \"1977\").idxmax() # We find the start index of the training data\n",
    "end_train   = (year_data_pd[\"DATE\"] == \"2011\").idxmax() #  *      *   end   index  *               *\n",
    "\n",
    "start_val = (year_data_pd[\"DATE\"] == \"2011\").idxmax() # We find the start index of the validation data\n",
    "end_val   = (year_data_pd[\"DATE\"] == \"2020\").idxmax() #  *      *   end   index  *                 *\n",
    "\n",
    "training_set = C_caen[start_train: end_train]\n",
    "validation_set = C_caen[start_val: end_val]\n",
    "\n",
    "training_set = np.array(training_set)\n",
    "validation_set = np.array(validation_set)\n",
    "\n",
    "# answer to b\n",
    "X = np.arange(0, len(training_set))\n",
    "Xm = sm.add_constant(X)\n",
    "model = sm.OLS(training_set, Xm)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "# answer to d\n",
    "training_pred = results.predict(Xm)\n",
    "sum = 0\n",
    "x = len(training_set)\n",
    "for i in range(x):\n",
    "    sum += abs(training_set[i] - training_pred[i])\n",
    "error = sum/x\n",
    "print(f\"Mean Absolute Error (MAE) between predicted and real consumptions: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.3. Use this model on the test set to forecast the electric daily production.\n",
    "\n",
    "a. Compare on a graph, the forecast to  real consumptions on the given period. \n",
    "\n",
    "b. Plot the errors of prediction. Are they acceptable?\n",
    "\n",
    "c. Compute the MAE on the test set and the $R^2$. Is the forecast reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# answer to a\n",
    "\n",
    "p, d, q = 10, 0, 0  \n",
    "model = ARIMA(validation_set, order=(p, d, q))\n",
    "arima_results = model.fit()\n",
    "\n",
    "forecast = arima_results.forecast(steps=len(validation_set))\n",
    "\n",
    "years = np.linspace(2011, 2019, num=len(validation_set))\n",
    "\n",
    "# predicted consumption(validation_pred) oscille toujours autour de 400, le pb doit se trouver avant\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(years, validation_set, label=\"Real Consumption\")\n",
    "plt.plot(years, forecast, label=\"Predicted Consumption\")\n",
    "plt.plot(years, validation_pred, label=\"Predicted Consumption\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Energy (Wh/m^2)\")\n",
    "plt.title(\"Comparison of Forecast vs. Real Consumption (Validation Set)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# answer to b \n",
    "\n",
    "validation_errors = validation_set - forecast\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(years, validation_errors, label=\"Prediction Errors\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.ylabel(\"Error (Wh/m^2)\")\n",
    "plt.title(\"Prediction Errors (Validation Set)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# answer to c\n",
    "\n",
    "sum = 0\n",
    "x = len(validation_set)\n",
    "for i in range(x):\n",
    "    sum += abs(validation_set[i] - forecast[i])\n",
    "error = sum/x\n",
    "print(f\"Mean Absolute Error (MAE) between predicted and real consumptions: {error}\")\n",
    "\n",
    "residuals = validation_set - forecast\n",
    "sse = np.sum(residuals ** 2)\n",
    "ssr = np.sum((validation_set - forecast.mean())**2)\n",
    "sst = ssr + sse\n",
    "r_squared = 1 - (sse / sst)\n",
    "print(f\"R^2 on the validation set: {r_squared}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.9.13"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.10.11"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
