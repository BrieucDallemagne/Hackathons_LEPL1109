{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon Stat\n",
    "\n",
    "This project illustrates the course LEPL1109 with an industrial application of statistics. You will analyse the capacity of solar production of electricity located in the French cities of Caen and Tours.\n",
    "The file 'radiation.csv' contains 3 columns \n",
    "DATE           : YYYYMMDD,\n",
    "Caen and Tours : the daily solar radiation in W/m2 measured in the 2 cities. \n",
    "Notice that data for some days are not reported due to failure of measurement system.\n",
    "\n",
    "## Report content\n",
    "\n",
    "•\tYou have to fill in this  jupyter notebook downloadable on the moodle website of the course\n",
    "\n",
    "•\tGrades are granted to the members whose names are in the Jupyter notebook. If your name doesn’t appear on the top of the notebook, you’ll get a 0, even though you are in a group on Moodle.\n",
    "\n",
    "•\tThe jupyter notebook must be compiled with printed results and next submitted via moodle. The absence of compiled results (or non-printed values) leads to a lower grade.\n",
    "\n",
    "## Report submission\n",
    "\n",
    "•\tThe deadline for submission is reported on the moodle website. Submission after the deadline will not be accepted.\n",
    "\n",
    "•\tTo submit your report, go to the section “APP” on Moodle and the subsection “Soumission du rapport”. You can upload your work there. Once you are sure that it is your final version, click the button “Envoyer le devoir”. It is important that you don’t forget to click on this button ! \n",
    "\n",
    "•\tReports that have not been uploaded through Moodle will not be corrected.\n",
    "\n",
    "## Names and Noma of participants:\n",
    "\n",
    "Part. 1: Lebras Floriane         (35022100)\n",
    "\n",
    "Part. 2: Martin Antoine           (86692100)\n",
    "\n",
    "Part. 3: Dallemagne Brieuc        (77122100)\n",
    "\n",
    "Part. 4: De Vleeschouwver Nora    (48602100)\n",
    "\n",
    "Part. 5: Debelle Thomas           (30002100)\n",
    "\n",
    "Part. 6: Orékhoff Alexandre       (54552100)\n",
    "\n",
    "---\n",
    "## 1. Energy calculation and basic statistics\n",
    "\n",
    "Compute the daily energy in WH per square meter of solar panel. For this purpose you use the datasets reporting the solar irradation measure in Tours and Tours (source https://www.ecad.eu/). The irradiation is measured in W/m2 per day. You will use the formula:\n",
    "\n",
    "C = E_Sol x 24 x P_cr x f_perf\n",
    "\n",
    "where  \n",
    "\n",
    "C is the electricity produced in WH/m2 for a day\n",
    "\n",
    "E_sol is the daily solar radiation in W/m2 \n",
    "\n",
    "P_cr is the peak power coefficient, set here to  0.18 (monocristal silicium)\n",
    "\n",
    "f_perf depends upon the system, set here to 0.75.\n",
    "\n",
    "Remark:\n",
    "\n",
    "1 W = 1 J/sec\n",
    "\n",
    "1 WH  is 1W x 3600sec = 3600J\n",
    "\n",
    "energy/m2 = E_sol * 24 * 3600 J/m2 = E_sol * 24 WH/m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1.1. Start by computing the daily energy in WH produced by a 1m2 solar panel\n",
    "\n",
    "a. Plot time-series of solar electric production in Caen and Tours from 1974 to 2023. Comment the evolution.\n",
    "\n",
    "b. Plot boxplots of daily productions for both cities. Comment the box plot.\n",
    "\n",
    "c. Remove outliers using the interquartile range. \n",
    "\n",
    "d. Plot an histogram of daily electricity production, after removal of outliers.\n",
    "\n",
    "Watchout: remove all days for which a outlier is observed in Caen **or** Tours to keep the same number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prérequis Import ###\n",
    "\n",
    "import csv\n",
    "import scipy as sp\n",
    "import scipy.stats as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv(\"Radiation.csv\")\n",
    "data_pd = data_pd.drop(columns=\"Unnamed: 0\")\n",
    "Caen = data_pd[\"Caen\"].to_numpy()\n",
    "Tours = data_pd[\"Tours\"].to_numpy()\n",
    "Date = data_pd[\"DATE\"].to_numpy()\n",
    "Index = np.arange(0, len(Date), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code 1.1 ###\n",
    "\n",
    "def time_to_year(dataset, month):\n",
    "    dataset = dataset.copy()\n",
    "    dataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"], format=\"%Y%m%d\")\n",
    "    \n",
    "    if month == -1:\n",
    "        dataset[\"DATE\"] = dataset[\"DATE\"].dt.strftime(\"%Y\") ## Make a copy if you want to keep the most interesting data\n",
    "        return dataset\n",
    "    \n",
    "    dataset[\"Year\"] = dataset[\"DATE\"].dt.year\n",
    "    dataset[\"Month\"] = dataset[\"DATE\"].dt.month\n",
    "    dataset = dataset[dataset[\"Month\"] == month]\n",
    "    dataset = dataset.drop(columns=[\"Year\", \"Month\"])    \n",
    "    \n",
    "    dataset[\"DATE\"] = dataset[\"DATE\"].dt.strftime(\"%Y\") ## Make a copy if you want to keep the most interesting data\n",
    "    return dataset\n",
    "\n",
    "def time_to_month(dataset):\n",
    "    dataset = dataset.copy()\n",
    "    dataset[\"DATE\"] = pd.to_datetime(dataset[\"DATE\"], format=\"%Y%m%d\")\n",
    "    dataset[\"DATE\"] = dataset[\"DATE\"].dt.strftime(\"%m\") ## Make a copy if you want to keep the most interesting data\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def box_plot(data1, data2, title, name1, name2):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot a box plot of two data\n",
    "\n",
    "    args:\n",
    "        data1: list of data\n",
    "        data2: list of data\n",
    "        title: title of the plot\n",
    "        name1: name of the first data\n",
    "        name2: name of the second data\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    bp = plt.boxplot([data1, data2], patch_artist=True, labels=[name1, name2])\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen']\n",
    "    for box, color in zip(bp['boxes'], colors):\n",
    "        box.set(facecolor=color)\n",
    "    \n",
    "    for whisker in bp['whiskers']:\n",
    "        whisker.set(color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    for flier in bp['fliers']:\n",
    "        flier.set(marker='o', markersize=5, markerfacecolor='red')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_data(data,title,Xname,Yname):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot a data\n",
    "    \n",
    "    args:\n",
    "        data: list of data\n",
    "        title: title of the plot\n",
    "        Xname: name of the X axis\n",
    "        Yname: name of the Y axis\n",
    "\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(Xname)\n",
    "    plt.ylabel(Yname)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def remove_outliers_interquartile(lst1,lst2,Date,Index):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        remove outliers from two lists of data by using the interquartile method\n",
    "\n",
    "    args:\n",
    "        lst1: np.array of data1\n",
    "        lst2: np.array of data2\n",
    "\n",
    "    return:\n",
    "        new_lst1: list of data1 without outliers\n",
    "        new_lst2: list of data2 without outliers\n",
    "        new_date: list of dates without outliers\n",
    "        new_index: list of indexes of the data without outliers\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(lst1, 25)\n",
    "    q3 = np.percentile(lst1, 75)\n",
    "    iqr = q3 - q1\n",
    "    Q1 = np.percentile(lst2,25)\n",
    "    Q3 = np.percentile(lst2,75)\n",
    "    IQR = Q3 - Q1\n",
    "    new_lst1 = list()\n",
    "    new_lst2 = list()\n",
    "    new_date = list()\n",
    "    new_index = list()\n",
    "    for i in range(len(lst1)):\n",
    "        if lst1[i] > q1 - 1.5*iqr and lst1[i] < q3 + 1.5*iqr and lst2[i] > Q1 - 1.5*IQR and lst2[i] < Q3 + 1.5*IQR:\n",
    "            new_lst1.append(lst1[i])\n",
    "            new_lst2.append(lst2[i])\n",
    "            new_date.append(Date[i])\n",
    "            new_index.append(Index[i])\n",
    "    return [new_lst1,new_lst2,new_date,new_index]\n",
    "\n",
    "def plot_histogram(data, title, x_label, y_label, color='skyblue'):\n",
    "    \"\"\"\n",
    "    description:\n",
    "        plot an histogram of a data\n",
    "    \n",
    "    args:\n",
    "        data: list of data\n",
    "        title: title of the plot\n",
    "        x_label: name of the X axis\n",
    "        y_label: name of the Y axis\n",
    "        color: color of the histogram\n",
    "    \n",
    "    Return: \n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(data, color=color, edgecolor='black', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "P_cr = 0.18\n",
    "f_perf = 0.75\n",
    "E_caen = Caen\n",
    "E_Tour = Tours\n",
    "C_caen = Caen*24*P_cr*f_perf\n",
    "C_Tour = Tours*24*P_cr*f_perf\n",
    "\n",
    "\n",
    "plot_data(C_caen,'C_caen','Jour','Electricité(W/m2)')\n",
    "plot_data(C_Tour,'C_Tour','Jour','Electricité(W/m2)')\n",
    "\n",
    "box_plot(C_caen,C_Tour,'C_caen et C_Tour','C_caen','C_Tour')\n",
    "\n",
    "res = remove_outliers_interquartile(C_caen,C_Tour,Date,Index)\n",
    "C_caen = res[0]\n",
    "C_Tour = res[1]\n",
    "Date = res[2]\n",
    "Index = res[3]\n",
    "\n",
    "plot_histogram(C_caen,'C_caen','Jour','Electricité(W/m2)')\n",
    "plot_histogram(C_Tour,'C_Tour','Jour','Electricité(W/m2)',color='lightgreen')\n",
    "\n",
    "\"\"\"\n",
    "commentaire 1.1.a: sur les deux premiers plots on peut voir que à part certaines valeurs grandement supérieurs vers la fin,\n",
    "les données des deux villes restent dans une range entre 0 et 1500.\n",
    "\n",
    "commentaire 1.1.b: les plots de boxplots nous confirment que la plupart valeurs sont dans une range entre 0 et 1500. \n",
    "On peut voir que du côté de Tour la range est un peu plus grande que celle de Caen. On peut aussi voir que la médiane\n",
    "de Tour est un tout petit peu plus grande que celle de Caen.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1.2. We want to compute monthly statistics of electricity solar production. Calculate for each city and for each month: \n",
    "\n",
    "1) the average daily production of electricity in Wh/m2\n",
    "\n",
    "2) the median daily production of electricity in Wh/m2\n",
    "\n",
    "3) the standard deviation daily production of electricity in Wh/m2\n",
    "\n",
    "4) the 5% percentile of daily production of electricity in Wh/m2\n",
    "\n",
    "5) the 95% percentile of daily production of electricity in Wh/m2\n",
    "\n",
    "Report the results in one or two tables. \n",
    "\n",
    "Compare and comment these statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### code 1.2 ###\n",
    "\n",
    "### Refaire avec les mois\n",
    "\n",
    "def q2_month(dataset, month):\n",
    "    \"\"\"Code that answer the question 1.2\n",
    "    description:\n",
    "        Compute the average, median, standard deviation, 5 percentile and 95 percentile of a dataset\n",
    "\n",
    "    Args:\n",
    "        dataset (Arraylist): An arraylist with the data of the daily production of electricity\n",
    "\n",
    "    Returns:\n",
    "        tuple: Return a tuple of data like: (average, median, standard deviation, 5 percentile, 95 percentile)\n",
    "    \"\"\"\n",
    "    dataset_av = np.sum(dataset) / len(dataset) # Average / mean\n",
    "\n",
    "    ### Comput median\n",
    "    dataset_median = np.median(dataset)\n",
    "\n",
    "    dataset_std = np.std(dataset)\n",
    "\n",
    "    dataset_5 = np.percentile(dataset, 5)\n",
    "    dataset_95 = np.percentile(dataset, 95)\n",
    "        \n",
    "    print(f\"{month}:  {dataset_av:.5f}\\t{dataset_median:.5f}\\t{dataset_std:.5f}\\t{dataset_5:.2f}\\t{dataset_95:.2f}\")\n",
    "    \n",
    "    return (dataset_av, dataset_median, dataset_std, dataset_5, dataset_95)\n",
    "\n",
    "def q2(dataset):\n",
    "    \"\"\"Answer Q2\n",
    "\n",
    "    Args:\n",
    "        dataset (pandas dataframe): a data frame with the data column being the month\n",
    "    \"\"\"\n",
    "    months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "    cities = [\"Caen\", \"Tours\"]\n",
    "    dataset = dataset.sort_values(by=[\"DATE\"])\n",
    "    \n",
    "    #print(dataset)\n",
    "    \n",
    "    for city in cities:\n",
    "        print(city)\n",
    "        print(\"Month: Average  Median \\t\\tStandard dev \\t5 quar  95 quartile\")\n",
    "        for month in months:\n",
    "            q2_month(dataset[dataset[\"DATE\"] == month][city].to_numpy(), month)\n",
    "        print(\"_______________\")\n",
    "\n",
    "clean_pd = pd.DataFrame({\"DATE\": Date,\"Caen\": C_caen, \"Tours\": C_Tour})\n",
    "month_pd = time_to_month(clean_pd.copy())\n",
    "q2(month_pd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "We can see how the power production drastically increase during summer (eg: June and July) and in the opposite dropping during winter.\n",
    "However there is a strong fluctuation in power production (standard deviation). It's normal since a bright sunny day produces way more than a grey and rainy day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fit of distributions and hypothesis tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.1. We focus on the daily production of electricity in April. Retrieve the data for month of April, in Caen and Tours. \n",
    "\n",
    " 1) Fit Gamma and normal distributions by log-likelihood maximization to \n",
    "    daily production of electricity during April (Caen & Tours).\n",
    "    \n",
    " 2) Compute the 4 log-likelihoods and select the best model for each location (justify your answer).\n",
    " \n",
    " 3) Compare on the same plot the empirical, the  gamma and normal pdf (the\n",
    "    empirical pdf is an histogram of frequencies).\n",
    "    \n",
    " 4) Why is there 3 parameters in python for the Gamma pdf whereas there\n",
    "    is only 2 in the distribution seen during lectures? \n",
    "\n",
    "Remark : set floc to -0.001 for the gamma.fit (to avoid troubles in case of null observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "april_pd = clean_pd.copy()\n",
    "april_pd[\"DATE\"] = pd.to_datetime(april_pd[\"DATE\"], format=\"%Y%m%d\")\n",
    "april_pd[\"DATE\"] = april_pd[\"DATE\"].dt.strftime(\"%m\")\n",
    "april_pd = april_pd.loc[april_pd[\"DATE\"] == \"04\"]\n",
    "\n",
    "april_Caen, april_Tours = april_pd[\"Caen\"].to_numpy(), april_pd[\"Tours\"].to_numpy()\n",
    "april_C_Caen = april_Caen*24*P_cr*f_perf\n",
    "april_C_Tours = april_Tours*24*P_cr*f_perf\n",
    "april_C_Caen, april_C_Tours, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "\n",
    "norm_fit_Caen_loc, norm_fit_Caen_scale = sp.stats.norm.fit(april_C_Caen)\n",
    "norm_fit_Tours_loc, norm_fit_Tours_scale = sp.stats.norm.fit(april_C_Tours)\n",
    "x_norm_Caen  = np.linspace(sp.stats.norm.ppf(0.01,loc=norm_fit_Caen_loc,scale=norm_fit_Caen_scale), sp.stats.norm.ppf(0.99,loc=norm_fit_Caen_loc,scale=norm_fit_Caen_scale), 100)\n",
    "x_norm_Tours = np.linspace(sp.stats.norm.ppf(0.01,loc=norm_fit_Tours_loc,scale=norm_fit_Tours_scale), sp.stats.norm.ppf(0.99,loc=norm_fit_Tours_loc,scale=norm_fit_Tours_scale), 100)\n",
    "norm_pdf_Caen = sp.stats.norm.pdf(x_norm_Caen,norm_fit_Caen_loc,norm_fit_Caen_scale)\n",
    "norm_pdf_Tours = sp.stats.norm.pdf(x_norm_Tours,norm_fit_Tours_loc,norm_fit_Tours_scale)\n",
    "normal_log_likelihood_maximisation_Caen = np.sum(np.log(norm_pdf_Caen))\n",
    "normal_log_likelihood_maximisation_Tours = np.sum(np.log(norm_pdf_Tours))\n",
    "\n",
    "\n",
    "gamma_fit_Caen_shape, gamma_fit_Caen_loc, gamma_fit_Caen_scale = sp.stats.gamma.fit(april_C_Caen,floc=-0.001)\n",
    "gamma_fit_Tours_shape, gamma_fit_Tours_loc, gamma_fit_Tours_scale = sp.stats.gamma.fit(april_C_Tours,floc=-0.001)\n",
    "x_gamma_Caen  = np.linspace(sp.stats.gamma.ppf(0.01,a= gamma_fit_Caen_shape,loc=gamma_fit_Caen_loc,scale=gamma_fit_Caen_scale), sp.stats.gamma.ppf(0.99,a=gamma_fit_Caen_shape,loc=gamma_fit_Caen_loc,scale=gamma_fit_Caen_scale), 100)\n",
    "x_gamma_Tours = np.linspace(sp.stats.gamma.ppf(0.01,a= gamma_fit_Tours_shape,loc=gamma_fit_Tours_loc,scale=gamma_fit_Tours_scale), sp.stats.gamma.ppf(0.99,a=gamma_fit_Tours_shape,loc=gamma_fit_Tours_loc,scale=gamma_fit_Tours_scale), 100)\n",
    "gamma_pdf_Caen = sp.stats.gamma.pdf(x_gamma_Caen,gamma_fit_Caen_shape,gamma_fit_Caen_loc,gamma_fit_Caen_scale)\n",
    "gamma_pdf_Tours = sp.stats.gamma.pdf(x_gamma_Tours,gamma_fit_Tours_shape,gamma_fit_Tours_loc,gamma_fit_Tours_scale)\n",
    "gamma_log_likelihood_maximisation_Caen = np.sum(np.log(gamma_pdf_Caen))\n",
    "gamma_log_likelihood_maximisation_Tours = np.sum(np.log(gamma_pdf_Tours))\n",
    "\n",
    "\n",
    "if normal_log_likelihood_maximisation_Caen > gamma_log_likelihood_maximisation_Caen:\n",
    "    log_likelihood_maximisation_Caen = normal_log_likelihood_maximisation_Caen\n",
    "    print(\"The normal distribution is the best model for Caen.\")\n",
    "else:\n",
    "    log_likelihood_maximisation_Caen = gamma_log_likelihood_maximisation_Caen\n",
    "    print(\"The Gamma distribution is the best model for Caen\")\n",
    "if normal_log_likelihood_maximisation_Tours > gamma_log_likelihood_maximisation_Tours:\n",
    "    log_likelihood_maximisation_Tours = normal_log_likelihood_maximisation_Tours\n",
    "    print(\"The normal distribution is the best model for Tours.\")\n",
    "else:\n",
    "    log_likelihood_maximisation_Tours = gamma_log_likelihood_maximisation_Tours\n",
    "    print(\"The Gamma distribution is the best model for Tours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2:\n",
    "\n",
    "The distribution model with a higher log likelihood maximisation is the better one because the higher is the log likelihood maximisation, the higher is the probability that the observed sample has been generated by this model and thus the closer we are to the real data.\n",
    "\n",
    "Here the log likelihood of the normal distribution is higher than the one with the Gamma distribution for Caen and Tours which is why we selected the normal distribution as the best model for both cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "ax1.set_title('Empirical, Gamma and Normal pdfs of the daily production of electricity in April in Caen')\n",
    "ax2.set_title('Empirical, Gamma and Normal pdfs of the daily production of electricity in April in Tours')\n",
    "ax1.set_xlabel(r'Daily production of electricity $[WH/m^2]$')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax2.set_xlabel('Daily production of electricity $[WH/m^2]$')\n",
    "ax2.set_ylabel('Probability')\n",
    "n, bins, patches = ax1.hist(april_C_Caen, 30, density=1,facecolor='skyblue', edgecolor='black', alpha=0.6, label='Empirical pdf Caen')\n",
    "n, bins, patches = ax2.hist(april_C_Tours, 30, density=1,facecolor='lightgreen', edgecolor='black', alpha=0.6, label='Empirical pdf Tours')\n",
    "ax1.plot(x_gamma_Caen, gamma_pdf_Caen,'r-', lw=2, alpha=0.6, label='Gamma pdf Caen')\n",
    "ax2.plot(x_gamma_Tours, gamma_pdf_Tours,'teal', lw=2, alpha=0.6, label='Gamma pdf Tours')\n",
    "ax1.plot(x_norm_Caen, norm_pdf_Caen,'k-', lw=2, alpha=0.6, label='Normal pdf Caen')\n",
    "ax2.plot(x_norm_Tours, norm_pdf_Tours,'coral', lw=2, alpha=0.6, label='Normal pdf Tours')\n",
    "ax1.legend()\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.4:\n",
    "\n",
    "The 3 parameters in python for the Gamma pdf are 'a', 'loc' and 'scale' and the 2 parameters in the distribution seen during lectures are 'alpha' and 'beta'.\n",
    "\n",
    "- 'a' is the shape of the distribution which corresponds to 'alpha' in lectures\n",
    "- 'loc' is the extra parameter we didn't see during the lectures. It is the location parameter\n",
    "- 'scale' corresponds to 1/beta in lectures\n",
    "\n",
    "The extra 'loc' parameter we have in python shifts the distribution around loc. Without loc and scale specified (loc=0 and scale=1 by default), the pdf is in the standardized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "2.2. Check if the average daily production in April is the same in Caen and Tours. Let us recall that the null hypothesis is\n",
    "\n",
    "$H_0$: $\\mu_{Caen} = \\mu_{Tours}$.\n",
    "\n",
    "Take care to comment your conclusions. Are all assumptions required to perform this test sastisfied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "\n",
    "\n",
    "\n",
    "Ttest=sp.stats.ttest_ind(X,Y)\n",
    "#I check if the pvalue(=smallest level of significance for which \n",
    "# the data indicate rejection of the null hypothesis) is bigger then alpha to keep H0\n",
    "#I choose alpha=5%\n",
    "if Ttest.pvalue<0.05:\n",
    "    print(\"H0 is rejected\")\n",
    "else:\n",
    "    print(\"H0 is not rejected\")\n",
    "\n",
    "#Having the same variance is required to perform this test and the populations need \n",
    "# to have a normal distribution and be i.i.d. . As we see in the first part of question 2, the normal \n",
    "# distribution is the best when we do a log likelihood maximisation. I test the equality \n",
    "# of variance at the next question and we can see that H0 is rejected so the variance are not the same.\n",
    "#H0 is rejected which means that the average daily production is not the same in Caen and Tours but the test is not valid since we don't respect the assumptions. That's why we have to use the Wilcoxon's test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.3. Test the equality of variance of daily production in April at Caen & Tours?\n",
    "$H_0$: $\\sigma_{Caen}=\\sigma_{Tours}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, april_date, april_index = remove_outliers_interquartile(april_C_Caen,april_C_Tours, Date, Index)\n",
    "\n",
    "n=len(X)\n",
    "S1    = np.std(X,ddof=1) \n",
    "S2    = np.std(Y,ddof=1)\n",
    "Tx     =S1**2/S2**2\n",
    "# I choose alpha=5%\n",
    "alpha = 0.05\n",
    "pval = sc.f.cdf(Tx,dfn=n-1 , dfd=n-1)\n",
    "if pval<alpha:\n",
    "    print(\"H0 is rejected\")\n",
    "else:\n",
    "    print(\"H0 is not rejected\")\n",
    "\n",
    "#H0 is rejected which means that the variance is not the same at Caen and Tours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.4. Explain the Wilcoxon's test. What is the main advantage of this test compared to the Student's T test. Why is this useful in our project? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wilcoxon's test is a non-parametric version of the Student's T test. It tests the null hypothesis that two populations from two matched samples have the same mean. The fact that it is non-parametric makes it not restricted by assumptions concerning the specific distribution of the populations and parameters such as the mean and variance.\n",
    "\n",
    "This is useful in our project because we have seen in questions 2.3 that the null hypothesis about the equality of variances in Caen and Tours was rejected and therefore without this assumption we couldn't test the null hypothesis about the equality of means with the Student's T test in question 2.2. With the Wilcoxon's test, we will finally be able to test the hypothesis that the two cities have the same mean without any restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2.5. Apply the Wilcoxon test to distributions of daily productions in April, at Caen and Tours.  What can you conclude about the means of daily production in these 2 cities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "d=np.subtract(april_C_Caen,april_C_Tours)\n",
    "wilcoxon_test = sp.stats.wilcoxon(d)\n",
    "print(\"The p-value of the Wilcoxon test is : \", wilcoxon_test.pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Wilcoxon's test, we can see that the p-value is extremely small and therefore the null hypothesis that said that the two cities had the same mean is very unlikely. With this we can conclude that the means of daily production in the two cities are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Regression and forecasting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.1. Do we observe any trend in the yearly solar production of electricity over the considered period?\n",
    "To answer this question: \n",
    "\n",
    "a. You will compute the average daily production (Wh/m2) during April from 1977 up to 2019 (included).\n",
    "\n",
    "b. You get a time-series of 43 values for each city. Regress these values on the explanatory\n",
    "variables X=(Year-1977). Don't forget to add a constant term and analyze results. \n",
    "\n",
    "c. Plot on the same graph, the predicted and the observed values.\n",
    "\n",
    "d. Comment your results! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New code because the previous one was producing an error\n",
    "year_data_pd = time_to_year(clean_pd, 4)           # Cast the data to year date \n",
    "Caen_av = [0]; Tours_av = [0]                       # Create two empty list for average\n",
    "years = np.arange(1977,2020); cities = [\"Tours\", \"Caen\"]\n",
    "Caen_av = []; Tours_av = []\n",
    "\n",
    "for year in years:\n",
    "    for city in cities:\n",
    "        num = 0\n",
    "        val = 0\n",
    "        for i in year_data_pd[year_data_pd[\"DATE\"] == str(year)][city]:\n",
    "            val += i\n",
    "            num += 1\n",
    "        if city == \"Tours\":\n",
    "            Tours_av.append(val/num)\n",
    "        else:\n",
    "            Caen_av.append(val/num)\n",
    "\n",
    "### Finish Average\n",
    "\n",
    "### B computing the linear regression X = (Year-1977)\n",
    "\n",
    "### Y = A(Year-1977) + C\n",
    "\n",
    "X = np.arange(0, len(years))\n",
    "\n",
    "coef_Caen = np.polyfit(X, Caen_av, 1)\n",
    "coef_Tours = np.polyfit(X, Tours_av, 1)\n",
    "\n",
    "fig, axe = plt.subplots()\n",
    "axe.plot(X, Caen_av, \"o\", color=\"green\", label=\"Consumption in Caen\", alpha=0.3)\n",
    "axe.plot(X, Tours_av, \"o\", color=\"blue\", label=\"Consumption in Tours\", alpha=0.3)\n",
    "axe.set_xlabel(\"Year since 1977\")\n",
    "axe.set_ylabel(r\"Average Daily Consumption [ $ \\frac{Wh}{m^2}$ ]\")\n",
    "axe.plot(X, coef_Caen[0]*X + coef_Caen[1], color=\"green\", label=\"Linear Regression for Caen\", linewidth=3)\n",
    "axe.plot(X, coef_Tours[0]*X + coef_Tours[1], color=\"blue\", label=\"Linear Regression for Tours\", linewidth=3)\n",
    "axe.grid()\n",
    "axe.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "We can see for both cities a certain rising trend. It is more subtle for *Tours* but can be clearly seen for *Caen*. We can link this raise of Average Daily Consumption with the electrification of our society. We can also explain this due to the importance of computers and digital in our society that requires more electricity. This growing market leads to more electrical consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.2. You want to design a model to forecast the solar electric production for the next day (location Caen only). You will work with data over the period 1977 to 2019. \n",
    "\n",
    "Let us denote by C(t) the production on day 't'. The model that we want to fit is called autoregressive and is defined as follows:\n",
    "\n",
    "$$C(t) = \\sum_{k=1}^{10} a_k C(t-k) $$\n",
    "\n",
    "This model is common in time-series analysis and predicts the production of the next day with the  recent observations.\n",
    "\n",
    "a. Split the dataset into a training set (1977 to 2010 included) and a validation set (2011 to 2019 included).\n",
    "\n",
    "b.\tEstimate this model with statsmodels on the training set. \n",
    "\n",
    "c.\tHow would you judge the quality of the predictive model? (Analyze statistics reported by statsmodel)\n",
    "\n",
    "d.\tCompute the Mean Absolute Error (MAE) between predicted and real consumptions (on the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### answer to a\n",
    "\n",
    "year_data_pd = time_to_year(clean_pd, -1)\n",
    "\n",
    "train_condition      = (year_data_pd[\"DATE\"] > \"1976\") & (year_data_pd[\"DATE\"] < \"2011\")\n",
    "validation_condition = (year_data_pd[\"DATE\"] > \"2010\") & (year_data_pd[\"DATE\"] < \"2020\") \n",
    "\n",
    "train_pd      = year_data_pd[train_condition]\n",
    "validation_pd = year_data_pd[validation_condition]\n",
    "\n",
    "start_train = (year_data_pd[\"DATE\"] == \"1977\").idxmax() # We find the start index of the training data\n",
    "end_train   = (year_data_pd[\"DATE\"] == \"2011\").idxmax() #  *      *   end   index  *               *\n",
    "\n",
    "start_val = (year_data_pd[\"DATE\"] == \"2011\").idxmax() # We find the start index of the validation data\n",
    "end_val   = (year_data_pd[\"DATE\"] == \"2020\").idxmax() #  *      *   end   index  *                 *\n",
    "\n",
    "training_set = C_caen[start_train: end_train]\n",
    "validation_set = C_caen[start_val: end_val]\n",
    "\n",
    "training_set = np.array(training_set)\n",
    "validation_set = np.array(validation_set)\n",
    "\n",
    "# answer to b\n",
    "\n",
    "training_set_series = pd.Series(training_set) # switch in panda Series object\n",
    "\n",
    "shifted_data = []\n",
    "for i in range(1, 11):\n",
    "    shifted_data.append(training_set_series.shift(i)) # shifted version on training_set\n",
    "\n",
    "X = pd.DataFrame(shifted_data).T # implementing the DataFrame\n",
    "Xm = sm.add_constant(X)\n",
    "model = sm.OLS(training_set[10:], Xm[10:])\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "# answer to d\n",
    "\n",
    "training_pred = results.predict(Xm[10:])\n",
    "error = np.mean(np.abs(training_set[10:] - training_pred))\n",
    "print(f\"Mean Absolute Error (MAE) between predicted and real consumptions: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality of the predictive model:\n",
    "\n",
    "    The ARIMA model shows low AIC and BIC values relatively to the number of parameters and log-likelihood which is indicating a good fit.\n",
    "    Its residuals have a standard deviation of about 163.805, implying some variability. All the lag coefficients are quite significant, showing well the past values influence. \n",
    "    We can also mention the presence of real and complex roots that shows us the dynamic model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3.3. Use this model on the test set to forecast the electric daily production.\n",
    "\n",
    "a. Compare on a graph, the forecast to  real consumptions on the given period. \n",
    "\n",
    "b. Plot the errors of prediction. Are they acceptable?\n",
    "\n",
    "c. Compute the MAE on the test set and the $R^2$. Is the forecast reliable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# answer to a\n",
    "\n",
    "# same method as 3.2 b\n",
    "\n",
    "validation_set_series = pd.Series(validation_set) # switch in panda Series \n",
    "validation_shifted_data = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    validation_shifted_data.append(validation_set_series.shift(i)) # shifted version on validation_set\n",
    "\n",
    "X_val = pd.DataFrame(validation_shifted_data).T # implementing the DataFrame\n",
    "Xm_val = sm.add_constant(X_val)\n",
    "model_val = sm.OLS(validation_set[10:], Xm_val[10:])\n",
    "results_validation_set = model_val.fit()\n",
    "\n",
    "forecast = results_validation_set.predict(Xm_val)\n",
    "\n",
    "years = np.linspace(2011, 2019, num=len(validation_set))\n",
    "\n",
    "validation_errors = abs(validation_set - forecast)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(years, validation_set, label=\"Actual Data\")\n",
    "plt.plot(years, forecast, label=\"Prediction of our Model\")\n",
    "plt.plot(years, validation_errors, label=\"Error\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(r\"solar electric production [ $ \\frac{Wh}{m^2}$ ]\")\n",
    "plt.title(\"Daily Solar Production: Forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# answer to c\n",
    "\n",
    "error_validation = np.mean(np.abs(validation_set - forecast))\n",
    "print(f\"Mean Absolute Error (MAE) between predicted and real consumptions: {error_validation}\")\n",
    "\n",
    "residuals = validation_set - forecast\n",
    "sse = np.sum(residuals ** 2)\n",
    "ssr = np.sum((validation_set - forecast.mean())**2)\n",
    "sst = ssr + sse\n",
    "r_squared = 1 - (sse / sst)\n",
    "print(f\"R^2 on the validation set: {r_squared}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors of prediction:\n",
    "\n",
    "    The errors of prediction are following the overall shape of the data and the predictions made.\n",
    "    They are proportionally to the data, decreasing when the production is increasing which shows that the model captures \n",
    "    the general trends and dependencies within the dataset.\n",
    "    But in case of periods of extreme variability or structural changes, we notice that the model may not perform as good \n",
    "    as usual.\n",
    "    Finally, we can conclude that they are acceptable when the production is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast Reliability:\n",
    "\n",
    "    Considering our R-squared value tending slightly towards 1, this suggest us a relatively good fit.\n",
    "    The Mean Absolute Error is also in an acceptable range even though it would be better to have it lower. \n",
    "    It would be recommended to approach the forecast with caution for the most part. \n",
    "    This is because, in certain cases, especially when dealing with low production levels, \n",
    "    the forecast might exhibit some bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.9.13"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.10.11"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}